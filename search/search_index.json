{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"OC-Accel Overview OpenCAPI Acceleration Framework , abbreviated as OC-Accel , the Integrated Development Environment (IDE) for creating application FPGA-based accelerators. By the nature of the FPGA, all of these types of accelerators are reconfigurable. The underlying enabling technology is OpenCAPI 3.0, the third generation coherent interface allowing processing elements (Code running on CPU cores and Logic implemented in FPGA chip) to seamlessly and coherently share host memory. All of the codes and materials are on Github https://github.com/OpenCAPI/oc-accel . The predecessor to OC-Accel was SNAP for CAPI 2.0 and CAPI 1.0. What is OpenCAPI OpenCAPI (Open Coherent Accelerator Processor Interface) is the third generation coherent interface and is an open standard for coherent high performance bus interface. Driven by emerging, accelerated heterogeneous computing and advanced memory and storage solutions, it provides the open interface that allows any microprocessor to attach to: Coherent user-level accelerators and I/O devices Advanced memories accessible via read/write or user-level DMA semantics Its specifications and ecosystem are managed by an open forum OpenCAPI Consortium . And the reference designs are opened on Github https://github.com/OpenCAPI/OpenCAPI3.0_Client_RefDesign . What can I do with OC-Accel OC-Accel helps to easily create FPGA-based acceleration engines with OpenCAPI interfaces. More details can be found in the \" User Guide \" tab for a more detailed step-to-step guide. Generally, creating an accelerator includes the steps as below: Develop your accelerator Many applications are developed using only software. However, there are classes of real-time applications in which the software cannot process the required data in a prescribed time period. Analysis tools such as statistical performance monitors or profiling tools can help to identify compute-intensive algorithms. These algorithms are sometimes called bottlenecks, hotspots or choke-points. Examples of such choke points are: streaming IO, highly complex math functions, operations using extremely large block processing, etc. Adding dedicated and customized hardware to offload these compute-intensive functions greatly improve throughput. These \"hot-spot\" function are moved to FPGA. This function is also mentioned as \" action \" in the following description. Software/hardware partition : After isolating the functions to run on FPGA side, the parameters need to be nailed down. It can be described as a job data structure. Learning the examples (See in \" Examples \" tab) as an start. A few libosnap API functions help you manipulate the FPGA card and the software/hardware interface. Work on the hardware action: Write the \"hardware action\" in a supported programming language, such as Vivado HLS or Verilog/VHDL. Together with the software part which invokes this hardware action, OC-Accel supports running co-simulation to verify the correctness. After the co-simulation is done, generate the FPGA bit image. The development environment for OC-Accel is Linux with Xilinx Vivado installed. You can also install other supported simulators to get better simulation speed. The FPGA card (target hardware) is NOT required during architecture development. Deploy it and run Deploy to Power9 server : Program the bit image to a real FPGA card. Compile the software code on Power9 and run! OC-Accel Framework Now let's have a glance at the diagram of OC-Accel framework. For more details about the directories, files and design hierarchy, see in repository structure page. The framework hardware consists of: TLx/DLx: Transaction layer and datalink layer of OpenCAPI device. cfg: Config subsystem of OpenCAPI snap_core: In Bridge mode, it provides the protocol translation for two directions. Module \"mmio\" converts TLx commands from Host Server to AXI4-Lite slave interface. Module \"bridge\" converts AXI4-MM commands from User logic \"Hardware Action\" to the host. Hardware action: also named \"action_wrapper\" is where developers implement their accelerator logic. People can take this open-source framework to add other interfaces (for example, NVMe, Ethernet, HBM, etc) depending on the capabilities of the FPGA card. The framework software consists of: libosnap: a few user-space functions to talk to upper applications. Developers can define them freely. libocxl: a few user-space functions talking to kernel module ocxl. ocxl: Linux kernel module to support OpenCAPI hardware. Already included with the OS distributions. For more information, please refer to \" Deep Dive \" tab on the menu bar. Dependencies Required tools for development Development is usually done on a Linux (x86) computer . Xilinx Vivado : OC-Accel currently supports Xilinx FPGA devices exclusively. For synthesis, simulation model and image build, the Xilinx Vivado 2018.3 or newer tool suites are recommended. Build process: Building the code and running the make environment requires the usual development tools gcc, make, sed, awk . If not installed already, the installer package build-essential will set up the most important tools. Configuring the OC-Accel framework will call a standalone tool that is based on the Linux kernel kconfig tool. The ncurses library must be installed to use the menu-driven user interface for kconfig . pyhton is optional but suggested to install. 2.7.x is fine. Simulators: You can use the build-in simulation xsim from Xilinx Vivado, or you can also use other simulators like Cadence irun or xcelium . For simulation, OC-Accel also relies on the xterm program. Supported FPGA cards OC-Accel framework needs a FPGA card with OpenCAPI interface, and a Slim-SAS cable to connect to a Power9 server. Today it supports: Alphadata 9V3 Alphadata 9H3 Alphadata 9H7 For FPGA vendors, it's easy to enable a new FPGA card with OpenCAPI interface to run OC-Accel, go to New board support page to learn how to. Supported Servers for deployment OpenCAPI interface needs the support on processor side. You can run OpenCAPI acceleration on Power9 servers with LaGrange or Monza processors installed. Today you can choose: LaGrange processor based systems: IPS FP5290 Wisrton Mihawk Monza processor based systems: IBM AC922 (an Acorn card is also required.) How to report an issue Submit an \"Issue\" on the GitHub. How to search information There are two ways: Use the \"Search\" button on the menu bar (up right). Or search the opened webpage by Ctrl+F . Git clone this repository and use grep or any of your favorite tools to search \"web-doc\" folder in a terminal. All of the contents on this website are plain text so you can search them easily. For example: cd web-doc grep KEYWORD * -r Compliance with SNAP1.0/2.0 OpenCAPI is actually the third generation of CAPI technology . That's why its version starts from OpenCAPI3.0. The same acceleration framework for CAPI1.0 and CAPI2.0 is also an open-source git repository at https://github.com/open-power/snap . Correspondingly, we call that SNAP1.0/2.0. The supported cards can be found here . SNAP1.0 runs on Power8 servers, with PCIe Gen3x8 cards. SNAP2.0 runs on Power9 servers, with PCIe Gen3x16 or PCIe Gen4x8 cards. OC-Accel runs on Power9 servers, using OpenCAPI x8 interface. Generally, your actions running on SNAP1.0/2.0 can be moved to OC-Accel directly without changing source-code. Check Migration Guide for more information. From FPGA to ASIC OC-Accel, with the OpenCAPI3.0 Device Reference designs together, allow people to move design from FPGA to ASIC easily, obtaining higher clock frequency and higher logic density. OC-Accel has implemented many scripts based on Vivado tool, but all of the components and workflow scripts are open-sourced. The steps of how to construct a project and perform software/hardware co-simulation are clear and easy to manipulate. After replacing some Xilinx IPs (like PLL, RAM, DDR Controller and PHY Serdes) with the selected Foundry IPs, the full oc_fpga_top design is suitable for the ASIC tape out, which brings an even higher-performance OpenCAPI ASIC device.","title":"Overview"},{"location":"#oc-accel-overview","text":"OpenCAPI Acceleration Framework , abbreviated as OC-Accel , the Integrated Development Environment (IDE) for creating application FPGA-based accelerators. By the nature of the FPGA, all of these types of accelerators are reconfigurable. The underlying enabling technology is OpenCAPI 3.0, the third generation coherent interface allowing processing elements (Code running on CPU cores and Logic implemented in FPGA chip) to seamlessly and coherently share host memory. All of the codes and materials are on Github https://github.com/OpenCAPI/oc-accel . The predecessor to OC-Accel was SNAP for CAPI 2.0 and CAPI 1.0.","title":"OC-Accel Overview"},{"location":"#what-is-opencapi","text":"OpenCAPI (Open Coherent Accelerator Processor Interface) is the third generation coherent interface and is an open standard for coherent high performance bus interface. Driven by emerging, accelerated heterogeneous computing and advanced memory and storage solutions, it provides the open interface that allows any microprocessor to attach to: Coherent user-level accelerators and I/O devices Advanced memories accessible via read/write or user-level DMA semantics Its specifications and ecosystem are managed by an open forum OpenCAPI Consortium . And the reference designs are opened on Github https://github.com/OpenCAPI/OpenCAPI3.0_Client_RefDesign .","title":"What is OpenCAPI"},{"location":"#what-can-i-do-with-oc-accel","text":"OC-Accel helps to easily create FPGA-based acceleration engines with OpenCAPI interfaces. More details can be found in the \" User Guide \" tab for a more detailed step-to-step guide. Generally, creating an accelerator includes the steps as below:","title":"What can I do with OC-Accel"},{"location":"#develop-your-accelerator","text":"Many applications are developed using only software. However, there are classes of real-time applications in which the software cannot process the required data in a prescribed time period. Analysis tools such as statistical performance monitors or profiling tools can help to identify compute-intensive algorithms. These algorithms are sometimes called bottlenecks, hotspots or choke-points. Examples of such choke points are: streaming IO, highly complex math functions, operations using extremely large block processing, etc. Adding dedicated and customized hardware to offload these compute-intensive functions greatly improve throughput. These \"hot-spot\" function are moved to FPGA. This function is also mentioned as \" action \" in the following description. Software/hardware partition : After isolating the functions to run on FPGA side, the parameters need to be nailed down. It can be described as a job data structure. Learning the examples (See in \" Examples \" tab) as an start. A few libosnap API functions help you manipulate the FPGA card and the software/hardware interface. Work on the hardware action: Write the \"hardware action\" in a supported programming language, such as Vivado HLS or Verilog/VHDL. Together with the software part which invokes this hardware action, OC-Accel supports running co-simulation to verify the correctness. After the co-simulation is done, generate the FPGA bit image. The development environment for OC-Accel is Linux with Xilinx Vivado installed. You can also install other supported simulators to get better simulation speed. The FPGA card (target hardware) is NOT required during architecture development.","title":"Develop your accelerator"},{"location":"#deploy-it-and-run","text":"Deploy to Power9 server : Program the bit image to a real FPGA card. Compile the software code on Power9 and run!","title":"Deploy it and run"},{"location":"#oc-accel-framework","text":"Now let's have a glance at the diagram of OC-Accel framework. For more details about the directories, files and design hierarchy, see in repository structure page. The framework hardware consists of: TLx/DLx: Transaction layer and datalink layer of OpenCAPI device. cfg: Config subsystem of OpenCAPI snap_core: In Bridge mode, it provides the protocol translation for two directions. Module \"mmio\" converts TLx commands from Host Server to AXI4-Lite slave interface. Module \"bridge\" converts AXI4-MM commands from User logic \"Hardware Action\" to the host. Hardware action: also named \"action_wrapper\" is where developers implement their accelerator logic. People can take this open-source framework to add other interfaces (for example, NVMe, Ethernet, HBM, etc) depending on the capabilities of the FPGA card. The framework software consists of: libosnap: a few user-space functions to talk to upper applications. Developers can define them freely. libocxl: a few user-space functions talking to kernel module ocxl. ocxl: Linux kernel module to support OpenCAPI hardware. Already included with the OS distributions. For more information, please refer to \" Deep Dive \" tab on the menu bar.","title":"OC-Accel Framework"},{"location":"#dependencies","text":"","title":"Dependencies"},{"location":"#required-tools-for-development","text":"Development is usually done on a Linux (x86) computer . Xilinx Vivado : OC-Accel currently supports Xilinx FPGA devices exclusively. For synthesis, simulation model and image build, the Xilinx Vivado 2018.3 or newer tool suites are recommended. Build process: Building the code and running the make environment requires the usual development tools gcc, make, sed, awk . If not installed already, the installer package build-essential will set up the most important tools. Configuring the OC-Accel framework will call a standalone tool that is based on the Linux kernel kconfig tool. The ncurses library must be installed to use the menu-driven user interface for kconfig . pyhton is optional but suggested to install. 2.7.x is fine. Simulators: You can use the build-in simulation xsim from Xilinx Vivado, or you can also use other simulators like Cadence irun or xcelium . For simulation, OC-Accel also relies on the xterm program.","title":"Required tools for development"},{"location":"#supported-fpga-cards","text":"OC-Accel framework needs a FPGA card with OpenCAPI interface, and a Slim-SAS cable to connect to a Power9 server. Today it supports: Alphadata 9V3 Alphadata 9H3 Alphadata 9H7 For FPGA vendors, it's easy to enable a new FPGA card with OpenCAPI interface to run OC-Accel, go to New board support page to learn how to.","title":"Supported FPGA cards"},{"location":"#supported-servers-for-deployment","text":"OpenCAPI interface needs the support on processor side. You can run OpenCAPI acceleration on Power9 servers with LaGrange or Monza processors installed. Today you can choose: LaGrange processor based systems: IPS FP5290 Wisrton Mihawk Monza processor based systems: IBM AC922 (an Acorn card is also required.)","title":"Supported Servers for deployment"},{"location":"#how-to-report-an-issue","text":"Submit an \"Issue\" on the GitHub.","title":"How to report an issue"},{"location":"#how-to-search-information","text":"There are two ways: Use the \"Search\" button on the menu bar (up right). Or search the opened webpage by Ctrl+F . Git clone this repository and use grep or any of your favorite tools to search \"web-doc\" folder in a terminal. All of the contents on this website are plain text so you can search them easily. For example: cd web-doc grep KEYWORD * -r","title":"How to search information"},{"location":"#compliance-with-snap1020","text":"OpenCAPI is actually the third generation of CAPI technology . That's why its version starts from OpenCAPI3.0. The same acceleration framework for CAPI1.0 and CAPI2.0 is also an open-source git repository at https://github.com/open-power/snap . Correspondingly, we call that SNAP1.0/2.0. The supported cards can be found here . SNAP1.0 runs on Power8 servers, with PCIe Gen3x8 cards. SNAP2.0 runs on Power9 servers, with PCIe Gen3x16 or PCIe Gen4x8 cards. OC-Accel runs on Power9 servers, using OpenCAPI x8 interface. Generally, your actions running on SNAP1.0/2.0 can be moved to OC-Accel directly without changing source-code. Check Migration Guide for more information.","title":"Compliance with SNAP1.0/2.0"},{"location":"#from-fpga-to-asic","text":"OC-Accel, with the OpenCAPI3.0 Device Reference designs together, allow people to move design from FPGA to ASIC easily, obtaining higher clock frequency and higher logic density. OC-Accel has implemented many scripts based on Vivado tool, but all of the components and workflow scripts are open-sourced. The steps of how to construct a project and perform software/hardware co-simulation are clear and easy to manipulate. After replacing some Xilinx IPs (like PLL, RAM, DDR Controller and PHY Serdes) with the selected Foundry IPs, the full oc_fpga_top design is suitable for the ASIC tape out, which brings an even higher-performance OpenCAPI ASIC device.","title":"From FPGA to ASIC"},{"location":"repository/","text":"This page introduces the components and files in OC-Accel. For a step-by-step guidance, please start from User Guide Steps in a glance . Repository Structure The diagram below shows the entire diretory structure of OC-Accel GIT repository. It links to another repository OpenCAPI3.0_Client_RefDesign or oc-bip which contains the card specific packages and modules to support OpenCAPI protocol. Sub directories in oc-accel The framework has some facilitating components: scripts : scripts for development environment. It displays a simple user interaction interface to select the card, the application to run (action), simulator, and other options. defconfig : configuration files for Jenkins regression test (users don't need them.) web-doc : documentations (this webpage) It has the modules to bridge OpenCAPI protocol: Software : provides user library to operate OpenCAPI cards like open_device(), attach_action(), etc. It includes header files and some tools. Hardware : It has a TLx-to-AXI bridge Verilog design in hdl , the scripts to build a Vivado project and run the process in setup , the simulation scripts in sim , and the link to oc-bip . Then it is the User Application actions directory. OC-Accel has already provided several examples in actions directory, including Verilog/VHDL examples and HLS (High Level Synthesis) examples. When a user wants to create a new Acceleration Application, he or she creates a new directory here. Under actions/<NAME> , use application also have software part sw , hardware part hw and test scripts test . Sub directories in oc-bip Any card vendor can add their card package support in oc-bip . The concept is similar to DSA (Device Support Archive) or BSP (Board Support Package). Board_support_packages : Card vendor need to create a separate folder for a new device. It includes: Constraint files (xdc) to describe the Card pins, flash interface, configurations and so on. Tcl files to create necessary Vivado IPs. Enprypted Verilog files to use Xilinx high speed serdes IOs. Verilog files for parameters and FPGA top. config_subsystem : Shared common logic for OpenCAPI Config. scripts : to pack the entire oc-bip to a Vivado IP (oc_bsp_wrap.xci). sim : Top Verilog file for simulation. Tlx : OpenCAPI Device transaction layer reference design. Dlx : OpenCAPI Device data link layer reference design. For more information, please refer to New Board Support . Filesets and Hardware Hierarchy Files used in Simulation Step In accelerator development, software and hardware co-simulation is a very important step. The simulation doesn't contain module tlx and dlx, but replies on OCSE to emulate the behavior of host. For more information, please refer to Co-Simulation . OCSE (OpenCAPI Simulation Engine) is also required. (TODO: update link) Top hierarchy in Simulation Step top.sv is in oc-bip/sim directory. oc_cfg is OpenCAPI Configuration subsystem. oc_function is the DUT (Design under Test) in this step. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw We use OCSE's DPI-C functions to drive and respond to the bus transactions. Files used in Implementation Step After co-simulation passed, it's time to do the Synthesis and Implementation in Vivado. For more information, please refer to User guide: build image . Top hierarchy in Implementation Step To generate a FPGA bitstream (binary image), the top design file is oc_fpga_top.v . oc_fpga_top is located in hardware/oc-bip/board_support_packeages/<CARD>/Verilog/framework_top oc_bsp_wrap includes TLx, Dlx, PHY, Flash subsystem and Card information (VPD). A script create_oc_bsp.tcl will assemble these components to a Vivado IP. oc_cfg is OpenCAPI Configuration subsystem. oc_function is what we have just simulated and proved that the functions can work correctly. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw Files used in Deployment When FPGA bit image is generated, use the tool oc-flash-script in oc-utils (TODO: update link) to download it from Power9 host server to the FPGA flash. After reboot, the bit image takes effect and you can ask application to call FPGA acceleration, with the help of libosnap and libocxl . libocxl need to be installed. Please follow the README file on its homepage. Application software and libosnap need to be compiled on Power9 host server also. For more information, please refer to User-guide: deploy .","title":"About the repository"},{"location":"repository/#repository-structure","text":"The diagram below shows the entire diretory structure of OC-Accel GIT repository. It links to another repository OpenCAPI3.0_Client_RefDesign or oc-bip which contains the card specific packages and modules to support OpenCAPI protocol.","title":"Repository Structure"},{"location":"repository/#sub-directories-in-oc-accel","text":"The framework has some facilitating components: scripts : scripts for development environment. It displays a simple user interaction interface to select the card, the application to run (action), simulator, and other options. defconfig : configuration files for Jenkins regression test (users don't need them.) web-doc : documentations (this webpage) It has the modules to bridge OpenCAPI protocol: Software : provides user library to operate OpenCAPI cards like open_device(), attach_action(), etc. It includes header files and some tools. Hardware : It has a TLx-to-AXI bridge Verilog design in hdl , the scripts to build a Vivado project and run the process in setup , the simulation scripts in sim , and the link to oc-bip . Then it is the User Application actions directory. OC-Accel has already provided several examples in actions directory, including Verilog/VHDL examples and HLS (High Level Synthesis) examples. When a user wants to create a new Acceleration Application, he or she creates a new directory here. Under actions/<NAME> , use application also have software part sw , hardware part hw and test scripts test .","title":"Sub directories in oc-accel"},{"location":"repository/#sub-directories-in-oc-bip","text":"Any card vendor can add their card package support in oc-bip . The concept is similar to DSA (Device Support Archive) or BSP (Board Support Package). Board_support_packages : Card vendor need to create a separate folder for a new device. It includes: Constraint files (xdc) to describe the Card pins, flash interface, configurations and so on. Tcl files to create necessary Vivado IPs. Enprypted Verilog files to use Xilinx high speed serdes IOs. Verilog files for parameters and FPGA top. config_subsystem : Shared common logic for OpenCAPI Config. scripts : to pack the entire oc-bip to a Vivado IP (oc_bsp_wrap.xci). sim : Top Verilog file for simulation. Tlx : OpenCAPI Device transaction layer reference design. Dlx : OpenCAPI Device data link layer reference design. For more information, please refer to New Board Support .","title":"Sub directories in oc-bip"},{"location":"repository/#filesets-and-hardware-hierarchy","text":"","title":"Filesets and Hardware Hierarchy"},{"location":"repository/#files-used-in-simulation-step","text":"In accelerator development, software and hardware co-simulation is a very important step. The simulation doesn't contain module tlx and dlx, but replies on OCSE to emulate the behavior of host. For more information, please refer to Co-Simulation . OCSE (OpenCAPI Simulation Engine) is also required. (TODO: update link)","title":"Files used in Simulation Step"},{"location":"repository/#top-hierarchy-in-simulation-step","text":"top.sv is in oc-bip/sim directory. oc_cfg is OpenCAPI Configuration subsystem. oc_function is the DUT (Design under Test) in this step. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw We use OCSE's DPI-C functions to drive and respond to the bus transactions.","title":"Top hierarchy in Simulation Step"},{"location":"repository/#files-used-in-implementation-step","text":"After co-simulation passed, it's time to do the Synthesis and Implementation in Vivado. For more information, please refer to User guide: build image .","title":"Files used in Implementation Step"},{"location":"repository/#top-hierarchy-in-implementation-step","text":"To generate a FPGA bitstream (binary image), the top design file is oc_fpga_top.v . oc_fpga_top is located in hardware/oc-bip/board_support_packeages/<CARD>/Verilog/framework_top oc_bsp_wrap includes TLx, Dlx, PHY, Flash subsystem and Card information (VPD). A script create_oc_bsp.tcl will assemble these components to a Vivado IP. oc_cfg is OpenCAPI Configuration subsystem. oc_function is what we have just simulated and proved that the functions can work correctly. oc_snap_core is in hardware/hdl User logic action_wrapper will be implemented in actions/<NAME>/hw","title":"Top hierarchy in Implementation Step"},{"location":"repository/#files-used-in-deployment","text":"When FPGA bit image is generated, use the tool oc-flash-script in oc-utils (TODO: update link) to download it from Power9 host server to the FPGA flash. After reboot, the bit image takes effect and you can ask application to call FPGA acceleration, with the help of libosnap and libocxl . libocxl need to be installed. Please follow the README file on its homepage. Application software and libosnap need to be compiled on Power9 host server also. For more information, please refer to User-guide: deploy .","title":"Files used in Deployment"},{"location":"actions-doc/hdl_example/","text":"hdl_example This is the 512b VHDL design inherited from SNAP1/2.","title":"hdl_example"},{"location":"actions-doc/hdl_example/#hdl_example","text":"This is the 512b VHDL design inherited from SNAP1/2.","title":"hdl_example"},{"location":"actions-doc/hdl_single_engine/","text":"hdl_single_engine A very simple example to interface with AXI-lite slave and AXI-master and measure the bandwidth and latency.","title":"hdl_single_engine"},{"location":"actions-doc/hdl_single_engine/#hdl_single_engine","text":"A very simple example to interface with AXI-lite slave and AXI-master and measure the bandwidth and latency.","title":"hdl_single_engine"},{"location":"actions-doc/hls_helloworld/","text":"hls_helloworld High Level Synthesis example. 512b, inherited from SNAP1/2.","title":"hls_helloworld"},{"location":"actions-doc/hls_helloworld/#hls_helloworld","text":"High Level Synthesis example. 512b, inherited from SNAP1/2.","title":"hls_helloworld"},{"location":"actions-doc/hls_memcopy_1024/","text":"hls_memcopy_1024 This is an action using HLS, 1024b. Also used to test the throughput of Host -> FPGA_RAM FPGA_RAM -> Host FPGA (DDR -> RAM) FPGA (RAM -> DDR) But HLS is not as efficient as hdl_single_engine. It has wasted some cycles between transactions. hw_test $ cd actions/hls_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR Temporal results: ( TODO : to be deleted) +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(DDR->RAM) FPGA(RAM->DDR) ------------------------------------------------------------------------------- 512 25.600 25.600 25.600 25.600 1024 51.200 51.200 51.200 48.762 2048 102.400 102.400 102.400 102.400 4096 204.800 204.800 204.800 215.579 8192 409.600 409.600 390.095 431.158 16384 780.190 780.190 862.316 780.190 32768 1560.381 1489.455 1560.381 1365.333 65536 2849.391 2621.440 2849.391 2340.571 131072 5242.880 4096.000 4519.724 3360.821 262144 8192.000 5957.818 6553.600 4369.067 524288 11650.844 7825.194 8738.133 5140.078 1048576 14768.676 8738.133 10082.462 5637.505 2097152 17331.835 9709.037 10979.853 5907.470 4194304 18808.538 10699.755 11554.556 6087.524 8388608 19737.901 12246.143 11848.316 6168.094 16777216 20164.923 15155.570 12000.870 6213.784 33554432 20177.049 17943.547 12082.979 6228.779 67108864 19872.332 19480.077 12120.077 6238.623 134217728 19790.287 20086.460 12138.711 6242.685 268435456 19907.702 20418.001 12146.401 6245.299 536870912 19959.510 20592.647 12151.349 6246.389 1073741824 19975.849 20682.689 12153.137 6246.607 Note There is an issue in the coding of the last column \"FPGA(RAM->DDR)\". It should be doubled. Hasn't fixed yet.","title":"hls_memcopy_1024"},{"location":"actions-doc/hls_memcopy_1024/#hls_memcopy_1024","text":"This is an action using HLS, 1024b. Also used to test the throughput of Host -> FPGA_RAM FPGA_RAM -> Host FPGA (DDR -> RAM) FPGA (RAM -> DDR) But HLS is not as efficient as hdl_single_engine. It has wasted some cycles between transactions.","title":"hls_memcopy_1024"},{"location":"actions-doc/hls_memcopy_1024/#hw_test","text":"$ cd actions/hls_memcopy_1024/tests $ sudo ./hw_throughput_test.sh -dINCR Temporal results: ( TODO : to be deleted) +-------------------------------------------------------------------------------+ | OC-Accel hls_memcopy_1024 Throughput (MBytes/s) | +-------------------------------------------------------------------------------+ bytes Host->FPGA_RAM FPGA_RAM->Host FPGA(DDR->RAM) FPGA(RAM->DDR) ------------------------------------------------------------------------------- 512 25.600 25.600 25.600 25.600 1024 51.200 51.200 51.200 48.762 2048 102.400 102.400 102.400 102.400 4096 204.800 204.800 204.800 215.579 8192 409.600 409.600 390.095 431.158 16384 780.190 780.190 862.316 780.190 32768 1560.381 1489.455 1560.381 1365.333 65536 2849.391 2621.440 2849.391 2340.571 131072 5242.880 4096.000 4519.724 3360.821 262144 8192.000 5957.818 6553.600 4369.067 524288 11650.844 7825.194 8738.133 5140.078 1048576 14768.676 8738.133 10082.462 5637.505 2097152 17331.835 9709.037 10979.853 5907.470 4194304 18808.538 10699.755 11554.556 6087.524 8388608 19737.901 12246.143 11848.316 6168.094 16777216 20164.923 15155.570 12000.870 6213.784 33554432 20177.049 17943.547 12082.979 6228.779 67108864 19872.332 19480.077 12120.077 6238.623 134217728 19790.287 20086.460 12138.711 6242.685 268435456 19907.702 20418.001 12146.401 6245.299 536870912 19959.510 20592.647 12151.349 6246.389 1073741824 19975.849 20682.689 12153.137 6246.607 Note There is an issue in the coding of the last column \"FPGA(RAM->DDR)\". It should be doubled. Hasn't fixed yet.","title":"hw_test"},{"location":"deep-dive/board-package/","text":"Enable a new FPGA card to OC-Accel Create a folder in hardware/oc-bip/board_support_packages/<NEW_CARD> Call the make process under hardware/oc-bip/<NEW_CARD> Add the CARD_TYPE, and other specific IPs in OC-Accel hardware/setup Modify the related CARD_TYPE related information in software Add the card choice in Kconfig Menu scripts/Kconfig TODO: More details to be put down.","title":"New Board Support"},{"location":"deep-dive/board-package/#enable-a-new-fpga-card-to-oc-accel","text":"Create a folder in hardware/oc-bip/board_support_packages/<NEW_CARD> Call the make process under hardware/oc-bip/<NEW_CARD> Add the CARD_TYPE, and other specific IPs in OC-Accel hardware/setup Modify the related CARD_TYPE related information in software Add the card choice in Kconfig Menu scripts/Kconfig TODO: More details to be put down.","title":"Enable a new FPGA card to OC-Accel"},{"location":"deep-dive/hardware-logic/","text":"OC-Accel Hardware Diagram and Clock Domain Here is a more detailed diagram for OC-Accel Bridge mode. oc_bsp_wrap and oc_cfg come from oc-bip ( OpenCAPI3.0 Device Reference design ) oc_function comes from OC-Accel hardware/hdl/core . It has following clock domains: clk_tlx : 400MHz (Don't change) clk_afu : 200MHz (Adjustable) clk_act : 200MHz (Adjustable) Other : Depending on the requirements of peripheral IPs AXI4 feature list Here lists the AXI4 feature list of oc_snap_core: AXI Lite-M Signal Width ADDR 32bits DATA 32bits AXI-S Signal Bit Width Comment ADDR (AWADDR/ARADDR) 64 Unaligned address supported DATA (WDATA/RDATA) 1024 ID (AWID/ARID/BID/RID) 1 to 5 2 to 32 AXI IDs USER (AWUSER/ARUSER) 9 Support up to 512 PASID (multi-process contexts) SIZE (AWSIZE/ARSIZE) 3 All sized transactions from 1 byte to 128bytes are supported BURST (AWBURST/ARBURST) 2 INCR RESP (BRESP/RRESP) 2 OKAY or ERROR WSTRB 128 All patterns are supported Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. AXI signals cache , lock , qos , region are not supported Warning Burst type \"FIXED\" is coded in snap_core but hasn't been tested. TLx feature list TLX to AFU Commands They are connected to mmio_wrapper. pr_mem_read (4B, 8B) pr_wr_mem (4B, 8B) intrp_rdy AFU to TLX Commands They are connected to bridge_wrapper. assign_actag rd_wnitc (128B, 64B) dma_w (128B, 64B) dma_pr_w (1B to 32B) rd_pr_wnitc (1B to 32B) intrp_req","title":"Hardware Logic"},{"location":"deep-dive/hardware-logic/#oc-accel-hardware","text":"","title":"OC-Accel Hardware"},{"location":"deep-dive/hardware-logic/#diagram-and-clock-domain","text":"Here is a more detailed diagram for OC-Accel Bridge mode. oc_bsp_wrap and oc_cfg come from oc-bip ( OpenCAPI3.0 Device Reference design ) oc_function comes from OC-Accel hardware/hdl/core . It has following clock domains: clk_tlx : 400MHz (Don't change) clk_afu : 200MHz (Adjustable) clk_act : 200MHz (Adjustable) Other : Depending on the requirements of peripheral IPs","title":"Diagram and Clock Domain"},{"location":"deep-dive/hardware-logic/#axi4-feature-list","text":"Here lists the AXI4 feature list of oc_snap_core:","title":"AXI4 feature list"},{"location":"deep-dive/hardware-logic/#axi-lite-m","text":"Signal Width ADDR 32bits DATA 32bits","title":"AXI Lite-M"},{"location":"deep-dive/hardware-logic/#axi-s","text":"Signal Bit Width Comment ADDR (AWADDR/ARADDR) 64 Unaligned address supported DATA (WDATA/RDATA) 1024 ID (AWID/ARID/BID/RID) 1 to 5 2 to 32 AXI IDs USER (AWUSER/ARUSER) 9 Support up to 512 PASID (multi-process contexts) SIZE (AWSIZE/ARSIZE) 3 All sized transactions from 1 byte to 128bytes are supported BURST (AWBURST/ARBURST) 2 INCR RESP (BRESP/RRESP) 2 OKAY or ERROR WSTRB 128 All patterns are supported Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. AXI signals cache , lock , qos , region are not supported Warning Burst type \"FIXED\" is coded in snap_core but hasn't been tested.","title":"AXI-S"},{"location":"deep-dive/hardware-logic/#tlx-feature-list","text":"","title":"TLx feature list"},{"location":"deep-dive/hardware-logic/#tlx-to-afu-commands","text":"They are connected to mmio_wrapper. pr_mem_read (4B, 8B) pr_wr_mem (4B, 8B) intrp_rdy","title":"TLX to AFU Commands"},{"location":"deep-dive/hardware-logic/#afu-to-tlx-commands","text":"They are connected to bridge_wrapper. assign_actag rd_wnitc (128B, 64B) dma_w (128B, 64B) dma_pr_w (1B to 32B) rd_pr_wnitc (1B to 32B) intrp_req","title":"AFU to TLX Commands"},{"location":"deep-dive/registers/","text":"OC-Accel Registers Configuration Registers link the software and hardware together. User application software code can use the provided libosnap APIs to read and write the registers implemented in FPGA logic, thus configure and control the functions in hardware. Those registers are also called MMIO (memory mapped IO) registers, because they are mapped into a large memory map. The \"addresses\" of these registers are 64bits wide. Memory map The OpenCAPI3.0 device memory map concepts (BAR, MMIO Global, MMIO Per PASID, and also memory space) are specified with respect to OpenCAPI configuration space specification . Here is a conceptual memory map: For OC-Accel: It only supports 1 AFU It only supports OpenCAPI3.0 C1 mode. MEM_SIZE = 0 It supports 512 PASIDs (User Process ID associated with a request) It only uses BAR0 Global MMIO Offset = 0, Size = 2GB Per PASID MMIO Offset = 2GB, Stride Size = 4MB So the above memory map is specified to: The settings can be found in: hardware/oc-bip/config_subsystem/cfg_descriptor.v Address Layout OC-Accel registers have two categories: Global Registers , 8B, defined in Global MMIO space. Use snap_global_read/write64() to access them. Action Registers , 4B, defined in Per PASID MMIO space. Use snap_action_read/write32() to access them. The higher 32bits of tlx_afu_cmd_pa (Physical Address) should be matched with BAR0. The lower 32bits, also called mmio_address, is processed in OC-Accel. Global Registers: Summary mmio_address[30:8] mmio_address [7:0] Abbr. Register Name 0x0 (Basic) 0x00 IVR Implementation Version Register 0x08 BDR Build Date Register 0x10 SCR SNAP Command Register 0x18 SSR SNAP Status Register 0x30 CAP Capacity Register 0x1A0 (Debug) 0x00 DBG_CLR Clear Debug Register 0x08 CNT_TLX_CMD Number of TLX Commands 0x10 CNT_TLX_RSP Number of TLX Responses 0x18 CNT_TLX_RTY Number of TLX Retry Responses 0x20 CNT_TLX_FAIL Number of TLX Fail Responses 0x28 CNT_TLX_XLP Number of TLX Translate Pending Responses 0x30 CNT_TLX_XLD Number of TLX Translate Done Responses 0x38 CNT_TLX_XLR Number of TLX Translate Retry Responses 0x40 CNT_AXI_CMD Number of total AXI Commands 0x48 CNT_AXI_RSP Number of total AXI Responses 0x50 BUF_CNT Counts in data buffers 0x58 TRAFIIC_IDLE No traffic over a period 0x60 TLX_IDLE_LIM Length of the period for TLX \"no traffic\" 0x68 AXI_IDLE_LIM Length of the period for AXI \"no traffic\" 0x1C0 (FIR) 0x00 FIFO_OVFL FIFO Overflow Status 0x08 FIR_TLX Errors on TLX interface Note FIR means \"Fault Isolation Register\". It usually means some errors happened. Global Registers: Details SNAP Basic Registers Implementation Version Register (IVR) Offset: 0x00 POR value depends on source for the build. Example for build based on commit with SHA ID eb43f4d80334d6a127af150345fed12dc5f45b7c and with distance 13 to SNAP Release v1.25.4: 0x0119040D_EB43F4D8 Bits Attributes Description 63..40 RO SNAP Release 63..56 RO Major release number 55..48 RO Intermediate release number 47..40 RO Minor release number 39..32 RO Distance of commit to SNAP release 31..0 RO First eight digits of SHA ID for commit Build Date Register (BDR) Offset: 0x08 POR value depends on build date and time. Example for build on January 12th, 2017 at 15:27: 0x00002017_01121527 Bits Attributes Description 63..48 RO Reserved 47.. 0 RO BCD coded build date and time 47..32 RO YYYY (year) 31..24 RO mm (month) 23..16 RO dd (day of month) 15..08 RO HH (hour) 07..00 RO MM (minute) SNAP Command Register (SCR) Offset: 0x10 Send SNAP commands via this register Bits Attributes Description 63..1 RO Reserved 0 WO soft reset to odma and action_wrapper SNAP Status Register (SSR) Offset: 0x18 Status of snap_core Bits Attributes Description 63..4 RO Reserved 3 RO SNAP fatal error: some bits are asserted in FIR registers 2 RO SNAP AXI side busy (?) 1 RO SNAP TLX side busy (?) 0 RO SNAP idle: Data buffers in snap_core are empty SNAP Capability Register (CAP) Offset: 0x20 Define the capability of the card Bitwise definition Bits Attributes Description 63..32 RO Reserved 31..16 RO Size of attached on-card SDRAM in MB 15..8 RO Reserved 7..0 RO Card type: 0x31 : AD9V3 0x32 : AD9H7 SNAP Debug Registers base_addr: 0x1A0 Note Subject to change. Debug Clear and Debug Counters DBG_CLR: Clear all of the following debug registers For following registers: bit[63:32] for Reads, bit [31:0] for Writes. CNT_TLX_CMD: Number of TLX Commands CNT_TLX_RSP: Number of TLX Responses CNT_TLX_RTY: Number of TLX Retry Responses CNT_TLX_FAIL: Number of TLX Fail Responses CNT_TLX_XLP: Number of TLX Translate Pending Responses CNT_TLX_XLD: Number of TLX Translate Done Responses CNT_TLX_XLR: Number of TLX Translate Retry Responses CNT_AXI_CMD: Number of total AXI Commands CNT_AXI_RSP: Number of total AXI Responses BUF_CNT: How many entries are valid for Read buffer and Write buffer Traffic Idle status TRAFFIC_IDLE: Used together with TLX_IDLE_LIM and AXI_IDLE_LIM. Bits Attributes Description 63..6 RO Reserved 5 RO tlx_cmd_idle in a certain number of cycles 4 RO tlx_rsp_idle in a certain number of cycles 3 RO axi_cmd_idle (Read) in a certain number of cycles 2 RO axi_rsp_idle (Read) in a certain number of cycles 1 RO axi_cmd_idle (Write) in a certain number of cycles 0 RO axi_rsp_idle (Write) in a certain number of cycles SNAP FIR Registers base_addr: 0x1C0 FIFO Overflow Status (FIFO_OVFL) offset: 0x00 Bits Attributes Description 63..6 RO Reserved 7 RO fir_fifo_overflow_cmdencw (Write Command Encoder) 6 RO fir_fifo_overflow_cmdencr (Read Command Encoder) 5 RO fir_fifo_overflow_cmdcnv (Command Clock Converter) 4 RO fir_fifo_overflow_rspcnv (Response Clock Converter) 3 RO fir_fifo_overflow_rspdecw (Write Response Decoder) 2 RO fir_fifo_overflow_rspdecr (Read Response Decoder) 1 RO fir_fifo_overflow_dbw (Write Data Buffer) 0 RO fir_fifo_overflow_dbr (Read Data Buffer) TLX Interface errors (FIR_TLX) offset: 0x08 Bits Attributes Description 63..3 RO Reserved 3 RO fir_tlx_response_unsupport 2 RO fir_tlx_rsp_err 1:0 RO fir_tlx_command_credit Action Registers: Summary mmio_address[30:22] means PASID. That means, the first process opens the OC Device, it will attach PASID=0 when it calls mmio_action_read/write32() . Meanwhile, the second process, the third process may attach PASID=1 and PASID=2 when they access the OC Device. Each process has its own \"process context\", and when OC Device wants to visit the host memory, it has to know which \"process context\" it belongs to, that means, the OC device needs to send the PASID with its commands. This PASID takes AWUSER or ARUSER as the vehicle to transfer from Action wrapper to snap_core. HDL design The user can freely define and implement the Action registers if it is written in Verilog/VHDL. However, there are still a registers that are recommended to implement. They are: mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Interrupt Enable Register 0x10 ATR Action Type Register 0x14 AVR Action Version Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x30 -> end Reserved and can be freely used Action Control Register (ACR) offset: 0x00 Bits Attributes Description 31..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO Action Done 0 RW Write 1 to start Action, Read it to know whether the action has been started Interrupt Enable Register (IER) offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt Action Type Register (ATR) offset: 0x10 Bits Attributes Description 31..0 RO Action Type, i.e, 0x10141008 Action Version Register (AVR) offset: 0x14 Bits Attributes Description 31..0 RO Action Release Version, user defined Interrupt Handle SRC Address Low (ISL) offset: 0x18 Bits Attributes Description 31..0 RO Interrupt Handle Source Address Low 32bits Interrupt Handle SRC Address High (ISH) offset: 0x1C Bits Attributes Description 31..0 RO Interrupt Handle Source Address High 32bits HLS design OC-Accel has already defined the Action Register Layout for HLS Actions. mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Global Interrupt Enable Register 0x08 IIE IP Interrupt Enable 0x0C IIS IP Interrupt Status 0x10 ATR Action Type Register 0x14 AVR Action Release Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x100 CONTROL1 sat + flags + seq 0x104 CONTROL2 Return Code 0x108 CONTROL3 Reserved 0x10C CONTROL4 Reserved 0x110 - 0x178 Job Data Registers (108 bytes) Note 0x00 to 0x0C are defined by Xilinx Document UG902. Action Control Register (ACR) offset: 0x00 Bits Attributes Description 31..8 RO Reserved 7 RW Auto Restart 6..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO/Clear on Read Action Done 0 RW/Clear on Read Start Action Global Interrupt Enable (IER) offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt IP Interrupt Enable (IIE) offset: 0x08 Bits Attributes Description 31..2 RO Reserved 1 RW Interrupt for ap_ready is enabled 0 RW Interrupt for ap_done is enabled IP Interrupt Status (IIS) offset: 0x0C Bits Attributes Description 31..2 RO Reserved 1 RW Status for ap_ready interrupt 0 RW Status for ap_done interrupt ATR, AVR, ISL, ISH These 4 registers have the same definitions as in HDL Action. offset: 0x10, ATR, Action Type Register offset: 0x14, AVR, Action Release Register offset: 0x18, ISL, Interrupt Handle SRC Address Low offset: 0x1C, ISH, Interrupt Handle SRC Address High HLS CONTROL Registers 4 registers are defined in actions/include/hls_snap.H . They take the addresses of 0x100, 0x104, 0x108 and 0x10C. typedef struct { snapu8_t sat; // short action type snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; // Priv_data } CONTROL; HLS Job DATA Registers 0x110 to 0x178 are user defined Job data structure. The size limit is 108 bytes. This is usually defined in actions/hls_<action_name>/include/<action_name>.h For example: typedef struct helloworld_job { struct snap_addr in; /* input data */ struct snap_addr out; /* offset table */ } helloworld_job_t;","title":"Registers"},{"location":"deep-dive/registers/#oc-accel-registers","text":"Configuration Registers link the software and hardware together. User application software code can use the provided libosnap APIs to read and write the registers implemented in FPGA logic, thus configure and control the functions in hardware. Those registers are also called MMIO (memory mapped IO) registers, because they are mapped into a large memory map. The \"addresses\" of these registers are 64bits wide.","title":"OC-Accel Registers"},{"location":"deep-dive/registers/#memory-map","text":"The OpenCAPI3.0 device memory map concepts (BAR, MMIO Global, MMIO Per PASID, and also memory space) are specified with respect to OpenCAPI configuration space specification . Here is a conceptual memory map: For OC-Accel: It only supports 1 AFU It only supports OpenCAPI3.0 C1 mode. MEM_SIZE = 0 It supports 512 PASIDs (User Process ID associated with a request) It only uses BAR0 Global MMIO Offset = 0, Size = 2GB Per PASID MMIO Offset = 2GB, Stride Size = 4MB So the above memory map is specified to: The settings can be found in: hardware/oc-bip/config_subsystem/cfg_descriptor.v","title":"Memory map"},{"location":"deep-dive/registers/#address-layout","text":"OC-Accel registers have two categories: Global Registers , 8B, defined in Global MMIO space. Use snap_global_read/write64() to access them. Action Registers , 4B, defined in Per PASID MMIO space. Use snap_action_read/write32() to access them. The higher 32bits of tlx_afu_cmd_pa (Physical Address) should be matched with BAR0. The lower 32bits, also called mmio_address, is processed in OC-Accel.","title":"Address Layout"},{"location":"deep-dive/registers/#global-registers-summary","text":"mmio_address[30:8] mmio_address [7:0] Abbr. Register Name 0x0 (Basic) 0x00 IVR Implementation Version Register 0x08 BDR Build Date Register 0x10 SCR SNAP Command Register 0x18 SSR SNAP Status Register 0x30 CAP Capacity Register 0x1A0 (Debug) 0x00 DBG_CLR Clear Debug Register 0x08 CNT_TLX_CMD Number of TLX Commands 0x10 CNT_TLX_RSP Number of TLX Responses 0x18 CNT_TLX_RTY Number of TLX Retry Responses 0x20 CNT_TLX_FAIL Number of TLX Fail Responses 0x28 CNT_TLX_XLP Number of TLX Translate Pending Responses 0x30 CNT_TLX_XLD Number of TLX Translate Done Responses 0x38 CNT_TLX_XLR Number of TLX Translate Retry Responses 0x40 CNT_AXI_CMD Number of total AXI Commands 0x48 CNT_AXI_RSP Number of total AXI Responses 0x50 BUF_CNT Counts in data buffers 0x58 TRAFIIC_IDLE No traffic over a period 0x60 TLX_IDLE_LIM Length of the period for TLX \"no traffic\" 0x68 AXI_IDLE_LIM Length of the period for AXI \"no traffic\" 0x1C0 (FIR) 0x00 FIFO_OVFL FIFO Overflow Status 0x08 FIR_TLX Errors on TLX interface Note FIR means \"Fault Isolation Register\". It usually means some errors happened.","title":"Global Registers: Summary"},{"location":"deep-dive/registers/#global-registers-details","text":"","title":"Global Registers: Details"},{"location":"deep-dive/registers/#snap-basic-registers","text":"","title":"SNAP Basic Registers"},{"location":"deep-dive/registers/#implementation-version-register-ivr","text":"Offset: 0x00 POR value depends on source for the build. Example for build based on commit with SHA ID eb43f4d80334d6a127af150345fed12dc5f45b7c and with distance 13 to SNAP Release v1.25.4: 0x0119040D_EB43F4D8 Bits Attributes Description 63..40 RO SNAP Release 63..56 RO Major release number 55..48 RO Intermediate release number 47..40 RO Minor release number 39..32 RO Distance of commit to SNAP release 31..0 RO First eight digits of SHA ID for commit","title":"Implementation Version Register (IVR)"},{"location":"deep-dive/registers/#build-date-register-bdr","text":"Offset: 0x08 POR value depends on build date and time. Example for build on January 12th, 2017 at 15:27: 0x00002017_01121527 Bits Attributes Description 63..48 RO Reserved 47.. 0 RO BCD coded build date and time 47..32 RO YYYY (year) 31..24 RO mm (month) 23..16 RO dd (day of month) 15..08 RO HH (hour) 07..00 RO MM (minute)","title":"Build Date Register (BDR)"},{"location":"deep-dive/registers/#snap-command-register-scr","text":"Offset: 0x10 Send SNAP commands via this register Bits Attributes Description 63..1 RO Reserved 0 WO soft reset to odma and action_wrapper","title":"SNAP Command Register (SCR)"},{"location":"deep-dive/registers/#snap-status-register-ssr","text":"Offset: 0x18 Status of snap_core Bits Attributes Description 63..4 RO Reserved 3 RO SNAP fatal error: some bits are asserted in FIR registers 2 RO SNAP AXI side busy (?) 1 RO SNAP TLX side busy (?) 0 RO SNAP idle: Data buffers in snap_core are empty","title":"SNAP Status Register (SSR)"},{"location":"deep-dive/registers/#snap-capability-register-cap","text":"Offset: 0x20 Define the capability of the card Bitwise definition Bits Attributes Description 63..32 RO Reserved 31..16 RO Size of attached on-card SDRAM in MB 15..8 RO Reserved 7..0 RO Card type: 0x31 : AD9V3 0x32 : AD9H7","title":"SNAP Capability Register (CAP)"},{"location":"deep-dive/registers/#snap-debug-registers","text":"base_addr: 0x1A0 Note Subject to change.","title":"SNAP Debug Registers"},{"location":"deep-dive/registers/#debug-clear-and-debug-counters","text":"DBG_CLR: Clear all of the following debug registers For following registers: bit[63:32] for Reads, bit [31:0] for Writes. CNT_TLX_CMD: Number of TLX Commands CNT_TLX_RSP: Number of TLX Responses CNT_TLX_RTY: Number of TLX Retry Responses CNT_TLX_FAIL: Number of TLX Fail Responses CNT_TLX_XLP: Number of TLX Translate Pending Responses CNT_TLX_XLD: Number of TLX Translate Done Responses CNT_TLX_XLR: Number of TLX Translate Retry Responses CNT_AXI_CMD: Number of total AXI Commands CNT_AXI_RSP: Number of total AXI Responses BUF_CNT: How many entries are valid for Read buffer and Write buffer","title":"Debug Clear and Debug Counters"},{"location":"deep-dive/registers/#traffic-idle-status","text":"TRAFFIC_IDLE: Used together with TLX_IDLE_LIM and AXI_IDLE_LIM. Bits Attributes Description 63..6 RO Reserved 5 RO tlx_cmd_idle in a certain number of cycles 4 RO tlx_rsp_idle in a certain number of cycles 3 RO axi_cmd_idle (Read) in a certain number of cycles 2 RO axi_rsp_idle (Read) in a certain number of cycles 1 RO axi_cmd_idle (Write) in a certain number of cycles 0 RO axi_rsp_idle (Write) in a certain number of cycles","title":"Traffic Idle status"},{"location":"deep-dive/registers/#snap-fir-registers","text":"base_addr: 0x1C0","title":"SNAP FIR Registers"},{"location":"deep-dive/registers/#fifo-overflow-status-fifo_ovfl","text":"offset: 0x00 Bits Attributes Description 63..6 RO Reserved 7 RO fir_fifo_overflow_cmdencw (Write Command Encoder) 6 RO fir_fifo_overflow_cmdencr (Read Command Encoder) 5 RO fir_fifo_overflow_cmdcnv (Command Clock Converter) 4 RO fir_fifo_overflow_rspcnv (Response Clock Converter) 3 RO fir_fifo_overflow_rspdecw (Write Response Decoder) 2 RO fir_fifo_overflow_rspdecr (Read Response Decoder) 1 RO fir_fifo_overflow_dbw (Write Data Buffer) 0 RO fir_fifo_overflow_dbr (Read Data Buffer)","title":"FIFO Overflow Status (FIFO_OVFL)"},{"location":"deep-dive/registers/#tlx-interface-errors-fir_tlx","text":"offset: 0x08 Bits Attributes Description 63..3 RO Reserved 3 RO fir_tlx_response_unsupport 2 RO fir_tlx_rsp_err 1:0 RO fir_tlx_command_credit","title":"TLX Interface errors (FIR_TLX)"},{"location":"deep-dive/registers/#action-registers-summary","text":"mmio_address[30:22] means PASID. That means, the first process opens the OC Device, it will attach PASID=0 when it calls mmio_action_read/write32() . Meanwhile, the second process, the third process may attach PASID=1 and PASID=2 when they access the OC Device. Each process has its own \"process context\", and when OC Device wants to visit the host memory, it has to know which \"process context\" it belongs to, that means, the OC device needs to send the PASID with its commands. This PASID takes AWUSER or ARUSER as the vehicle to transfer from Action wrapper to snap_core.","title":"Action Registers: Summary"},{"location":"deep-dive/registers/#hdl-design","text":"The user can freely define and implement the Action registers if it is written in Verilog/VHDL. However, there are still a registers that are recommended to implement. They are: mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Interrupt Enable Register 0x10 ATR Action Type Register 0x14 AVR Action Version Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x30 -> end Reserved and can be freely used","title":"HDL design"},{"location":"deep-dive/registers/#action-control-register-acr","text":"offset: 0x00 Bits Attributes Description 31..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO Action Done 0 RW Write 1 to start Action, Read it to know whether the action has been started","title":"Action Control Register (ACR)"},{"location":"deep-dive/registers/#interrupt-enable-register-ier","text":"offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt","title":"Interrupt Enable Register (IER)"},{"location":"deep-dive/registers/#action-type-register-atr","text":"offset: 0x10 Bits Attributes Description 31..0 RO Action Type, i.e, 0x10141008","title":"Action Type Register (ATR)"},{"location":"deep-dive/registers/#action-version-register-avr","text":"offset: 0x14 Bits Attributes Description 31..0 RO Action Release Version, user defined","title":"Action Version Register (AVR)"},{"location":"deep-dive/registers/#interrupt-handle-src-address-low-isl","text":"offset: 0x18 Bits Attributes Description 31..0 RO Interrupt Handle Source Address Low 32bits","title":"Interrupt Handle SRC Address Low (ISL)"},{"location":"deep-dive/registers/#interrupt-handle-src-address-high-ish","text":"offset: 0x1C Bits Attributes Description 31..0 RO Interrupt Handle Source Address High 32bits","title":"Interrupt Handle SRC Address High (ISH)"},{"location":"deep-dive/registers/#hls-design","text":"OC-Accel has already defined the Action Register Layout for HLS Actions. mmio_address [21:0] Abbr. Register Name 0x00 ACR Action Control Register 0x04 IER Global Interrupt Enable Register 0x08 IIE IP Interrupt Enable 0x0C IIS IP Interrupt Status 0x10 ATR Action Type Register 0x14 AVR Action Release Register 0x18 ISL Interrupt Handle SRC Address Low 0x1C ISH Interrupt Handle SRC Address High 0x100 CONTROL1 sat + flags + seq 0x104 CONTROL2 Return Code 0x108 CONTROL3 Reserved 0x10C CONTROL4 Reserved 0x110 - 0x178 Job Data Registers (108 bytes) Note 0x00 to 0x0C are defined by Xilinx Document UG902.","title":"HLS design"},{"location":"deep-dive/registers/#action-control-register-acr_1","text":"offset: 0x00 Bits Attributes Description 31..8 RO Reserved 7 RW Auto Restart 6..4 RO Reserved 3 RO Action Ready 2 RO Action Idle 1 RO/Clear on Read Action Done 0 RW/Clear on Read Start Action","title":"Action Control Register (ACR)"},{"location":"deep-dive/registers/#global-interrupt-enable-ier","text":"offset: 0x04 Bits Attributes Description 31..1 RO Reserved 0 RW Enable Interrupt","title":"Global Interrupt Enable (IER)"},{"location":"deep-dive/registers/#ip-interrupt-enable-iie","text":"offset: 0x08 Bits Attributes Description 31..2 RO Reserved 1 RW Interrupt for ap_ready is enabled 0 RW Interrupt for ap_done is enabled","title":"IP Interrupt Enable (IIE)"},{"location":"deep-dive/registers/#ip-interrupt-status-iis","text":"offset: 0x0C Bits Attributes Description 31..2 RO Reserved 1 RW Status for ap_ready interrupt 0 RW Status for ap_done interrupt","title":"IP Interrupt Status (IIS)"},{"location":"deep-dive/registers/#atr-avr-isl-ish","text":"These 4 registers have the same definitions as in HDL Action. offset: 0x10, ATR, Action Type Register offset: 0x14, AVR, Action Release Register offset: 0x18, ISL, Interrupt Handle SRC Address Low offset: 0x1C, ISH, Interrupt Handle SRC Address High","title":"ATR, AVR, ISL, ISH"},{"location":"deep-dive/registers/#hls-control-registers","text":"4 registers are defined in actions/include/hls_snap.H . They take the addresses of 0x100, 0x104, 0x108 and 0x10C. typedef struct { snapu8_t sat; // short action type snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; // Priv_data } CONTROL;","title":"HLS CONTROL Registers"},{"location":"deep-dive/registers/#hls-job-data-registers","text":"0x110 to 0x178 are user defined Job data structure. The size limit is 108 bytes. This is usually defined in actions/hls_<action_name>/include/<action_name>.h For example: typedef struct helloworld_job { struct snap_addr in; /* input data */ struct snap_addr out; /* offset table */ } helloworld_job_t;","title":"HLS Job DATA Registers"},{"location":"deep-dive/software-api/","text":"OC-Accel Software Environment Variables To debug libsnap functionality or associated actions, there are currently some environment variables available: SNAP_TRACE : 0x1 General libsnap trace 0x2 Enable register read/write trace 0x4 Enable simulation specific trace 0x8 Enable action traces. For example, use SNAP_TRACE=0xF to enable all above. Applications might use more bits above those defined here. Tools snap_maint: Currently it just prints information. snap_peek: debug tools to read MMIO registers. snap_poke: debug tools to write MMIO registers. APIs Refer to: include/libosnap.h lib/osnap.c","title":"Software API"},{"location":"deep-dive/software-api/#oc-accel-software","text":"","title":"OC-Accel Software"},{"location":"deep-dive/software-api/#environment-variables","text":"To debug libsnap functionality or associated actions, there are currently some environment variables available: SNAP_TRACE : 0x1 General libsnap trace 0x2 Enable register read/write trace 0x4 Enable simulation specific trace 0x8 Enable action traces. For example, use SNAP_TRACE=0xF to enable all above. Applications might use more bits above those defined here.","title":"Environment Variables"},{"location":"deep-dive/software-api/#tools","text":"snap_maint: Currently it just prints information. snap_peek: debug tools to read MMIO registers. snap_poke: debug tools to write MMIO registers.","title":"Tools"},{"location":"deep-dive/software-api/#apis","text":"Refer to: include/libosnap.h lib/osnap.c","title":"APIs"},{"location":"misc/doc-guide/","text":"How to generate this website This static documentation website is created by MkDocs and is using a theme from bootswatch . It uses \"github pages\" and this site is hosted by Github. The documentation source files are written in Markdown format. With MkDocs tool, the generated site files (html files) are automatically pushed into a specific branch gh-pages of the git repository. Installation 1. Install python and pip python and pip 2. Install mkdocs-bootswatch pip install mkdocs-bootswatch Please refer to bootswatch for more information. 3. Install a markdown editor You can simply edit the markdown (.md) files by any text editor, but it's better to user a professional markdown editor. typora . It supports all of the platforms (Windows/MacOS/Linux). Please configure typora to strict Markdown mode. That ensures you get the same output effects on both typora and mkdocs . vscode . It's also a good editor and has abundant functions and extensions. You can install extensions of Markdown, Preview and Spell checker. 4. Install other optional tools pdf2svg: This tool can convert a pdf lossless picture to svg format. For Mac OS, it can be easily installed by Homebrew , simply by brew install pdf2svg . Alternative choice is Inkscape which is a free drawing tool and can help you draw and convert vector graphics. Website Structure First, you need to git clone the oc-accel repository and go to web-doc directory. Make sure you are working on a branch other than master. git clone git@github.com:OpenCAPI/oc-accel.git git checkout <A branch other than master> cd oc-accel/web-doc The docs folder is where to put the markdown files, and the mkdocs.yml lists the website structure and global definitons. For example, this site has a structure like: nav: - Home: 'index.md' - User Guide: - 'Prepare Environment': 'user-guide/prepare-env.md' - 'Run an example': 'user-guide/run-example.md' - 'Create a new action': 'user-guide/new-action.md' - 'Co-Simulation': 'user-guide/co-simulation.md' - 'FPGA Image build': 'user-guide/make-image.md' - 'Optimize HLS action': 'user-guide/optimize-hls.md' - 'Deploy on Power Server': 'user-guide/deploy.md' - 'Debug an issue': 'user-guide/debug-issue.md' - 'Command Reference': 'user-guide/command-reference.md' - Examples: - 'hdl_example': 'actions-doc/hdl-example.md' - 'hdl_helloworld': 'actions-doc/hdl-helloworld.md' - 'hls_helloworld': 'actions-doc/hls-helloworld.md' - 'hls_memcopy': 'actions-doc/hls-memcopy.md' - Deep Dive: - 'SNAP Software API': 'deep-dive/libosnap.md' - 'SNAP Registers': 'deep-dive/registers.md' - 'SNAP Logic Design': 'deep-dive/snap_core.md' - 'New Board Support': 'deep-dive/board-package.md' - Misc: - 'Document Guide': 'misc/doc-guide.md' You can edit them as needed. Write Markdown pages On your local desktop, edit markdown files under web-doc/docs folder. If you want to add/delete/rename the files, you also need to edit mkdocs.yml Now it's time to work with an editor (i.e, typora) to write the documents. You also may need to learn some markdown syntax. Don't worry, that's easy. And please turn on the \"spell checking\" in your Markdown editor. In your terminal (MacOS or Linux), or cmd (Windows), start a serve process: mkdocs serve Then open a web browser, input http://127.0.0.1:8000 . So whenever you save any markdown files, you can check the generated website immediately. Play with pictures The first rule Reduce the usage of pictures. Avoid unnecessary screenshots. It's quite easy You can insert jpg, png, svg files. You can also simply copy paste pictures from clipboard and paste them. Copy the files into a directory ./${filename}.assets , and here ${filename} is the name of markdown file. Use relative links in the document. Note If you are using Typora, please enable \"Copy images into ./${filename}.assets folder\" in Preferences of typora. Tools to draw diagrams You can take any drawing tools to create diagrams. You can save them as PNG format, but the better way is to save to SVG format. For the diagrams from Microsoft PowerPoint, you can select the region of a diagram in PPT, Ctrl-C to copy it, and Ctrl-V to paste it in Typora directly. In this case, the diagram is saved as an PNG file. But there is a better way to get the smallest file size and best quality: In PowerPoint, select the region of diagram, right-click mouse -> \"Save as Picture ...\" and save it as \"PDF\" format. Open the PDF file with Inkscape . (Right-click the file -> \"Open with ...\", choose Inkscape in the poped up list). Unclick \"Embed images\" and then \"OK\". In Inkscape, save it as SVG file. Insert the SVG file into Typora. In my experiment, the PNG file is 188KB. But with the above flow to save it as SVG file, its size is 62KB. As a vectored diagram, it doesn't have any quality loss when zooming in. Warning Please use normal fonts in PPT, for example \"Arial\". Otherwise you may get a SVG file with a replaced font and that may look different. Code blocks and Admonitions Code blocks Please assign the code language so it can be correctly rendered. For example ``` C for C language. // A function to implement bubble sort void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) // Last i elements are already in place for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); } Admonitions You can use !!! Note or !!! Warning or !!! Danger to start a paragraph of admonitions. Then use 4 spaces to start the admonition text. For example !!! Danger \"Error Message\" This is a dangerous error. It will be shown as: Error Message This is a dangerous error. Deploy to Github Pages When most of the edition work is done, and it's time to commit your documents to oc-snap github. First, you should commit and push your changes of source files (in web-doc ) to git repository. Create pull request, ask someone to review the documents, merge them into master branch after getting approvements. Then you can simply publish website with just one step: cd <PATH>/oc-accel/web-doc mkdocs gh-deploy The entire website will be pushed to gh-pages branch of oc-snap repository. The documentation website will be available at https://opencapi.github.io/oc-accel/ !","title":"Document Guide"},{"location":"misc/doc-guide/#how-to-generate-this-website","text":"This static documentation website is created by MkDocs and is using a theme from bootswatch . It uses \"github pages\" and this site is hosted by Github. The documentation source files are written in Markdown format. With MkDocs tool, the generated site files (html files) are automatically pushed into a specific branch gh-pages of the git repository.","title":"How to generate this website"},{"location":"misc/doc-guide/#installation","text":"","title":"Installation"},{"location":"misc/doc-guide/#1-install-python-and-pip","text":"python and pip","title":"1. Install python and pip"},{"location":"misc/doc-guide/#2-install-mkdocs-bootswatch","text":"pip install mkdocs-bootswatch Please refer to bootswatch for more information.","title":"2. Install mkdocs-bootswatch"},{"location":"misc/doc-guide/#3-install-a-markdown-editor","text":"You can simply edit the markdown (.md) files by any text editor, but it's better to user a professional markdown editor. typora . It supports all of the platforms (Windows/MacOS/Linux). Please configure typora to strict Markdown mode. That ensures you get the same output effects on both typora and mkdocs . vscode . It's also a good editor and has abundant functions and extensions. You can install extensions of Markdown, Preview and Spell checker.","title":"3. Install a markdown editor"},{"location":"misc/doc-guide/#4-install-other-optional-tools","text":"pdf2svg: This tool can convert a pdf lossless picture to svg format. For Mac OS, it can be easily installed by Homebrew , simply by brew install pdf2svg . Alternative choice is Inkscape which is a free drawing tool and can help you draw and convert vector graphics.","title":"4. Install other optional tools"},{"location":"misc/doc-guide/#website-structure","text":"First, you need to git clone the oc-accel repository and go to web-doc directory. Make sure you are working on a branch other than master. git clone git@github.com:OpenCAPI/oc-accel.git git checkout <A branch other than master> cd oc-accel/web-doc The docs folder is where to put the markdown files, and the mkdocs.yml lists the website structure and global definitons. For example, this site has a structure like: nav: - Home: 'index.md' - User Guide: - 'Prepare Environment': 'user-guide/prepare-env.md' - 'Run an example': 'user-guide/run-example.md' - 'Create a new action': 'user-guide/new-action.md' - 'Co-Simulation': 'user-guide/co-simulation.md' - 'FPGA Image build': 'user-guide/make-image.md' - 'Optimize HLS action': 'user-guide/optimize-hls.md' - 'Deploy on Power Server': 'user-guide/deploy.md' - 'Debug an issue': 'user-guide/debug-issue.md' - 'Command Reference': 'user-guide/command-reference.md' - Examples: - 'hdl_example': 'actions-doc/hdl-example.md' - 'hdl_helloworld': 'actions-doc/hdl-helloworld.md' - 'hls_helloworld': 'actions-doc/hls-helloworld.md' - 'hls_memcopy': 'actions-doc/hls-memcopy.md' - Deep Dive: - 'SNAP Software API': 'deep-dive/libosnap.md' - 'SNAP Registers': 'deep-dive/registers.md' - 'SNAP Logic Design': 'deep-dive/snap_core.md' - 'New Board Support': 'deep-dive/board-package.md' - Misc: - 'Document Guide': 'misc/doc-guide.md' You can edit them as needed.","title":"Website Structure"},{"location":"misc/doc-guide/#write-markdown-pages","text":"On your local desktop, edit markdown files under web-doc/docs folder. If you want to add/delete/rename the files, you also need to edit mkdocs.yml Now it's time to work with an editor (i.e, typora) to write the documents. You also may need to learn some markdown syntax. Don't worry, that's easy. And please turn on the \"spell checking\" in your Markdown editor. In your terminal (MacOS or Linux), or cmd (Windows), start a serve process: mkdocs serve Then open a web browser, input http://127.0.0.1:8000 . So whenever you save any markdown files, you can check the generated website immediately.","title":"Write Markdown pages"},{"location":"misc/doc-guide/#play-with-pictures","text":"","title":"Play with pictures"},{"location":"misc/doc-guide/#the-first-rule","text":"Reduce the usage of pictures. Avoid unnecessary screenshots.","title":"The first rule"},{"location":"misc/doc-guide/#its-quite-easy","text":"You can insert jpg, png, svg files. You can also simply copy paste pictures from clipboard and paste them. Copy the files into a directory ./${filename}.assets , and here ${filename} is the name of markdown file. Use relative links in the document. Note If you are using Typora, please enable \"Copy images into ./${filename}.assets folder\" in Preferences of typora.","title":"It's quite easy"},{"location":"misc/doc-guide/#tools-to-draw-diagrams","text":"You can take any drawing tools to create diagrams. You can save them as PNG format, but the better way is to save to SVG format. For the diagrams from Microsoft PowerPoint, you can select the region of a diagram in PPT, Ctrl-C to copy it, and Ctrl-V to paste it in Typora directly. In this case, the diagram is saved as an PNG file. But there is a better way to get the smallest file size and best quality: In PowerPoint, select the region of diagram, right-click mouse -> \"Save as Picture ...\" and save it as \"PDF\" format. Open the PDF file with Inkscape . (Right-click the file -> \"Open with ...\", choose Inkscape in the poped up list). Unclick \"Embed images\" and then \"OK\". In Inkscape, save it as SVG file. Insert the SVG file into Typora. In my experiment, the PNG file is 188KB. But with the above flow to save it as SVG file, its size is 62KB. As a vectored diagram, it doesn't have any quality loss when zooming in. Warning Please use normal fonts in PPT, for example \"Arial\". Otherwise you may get a SVG file with a replaced font and that may look different.","title":"Tools to draw diagrams"},{"location":"misc/doc-guide/#code-blocks-and-admonitions","text":"","title":"Code blocks and Admonitions"},{"location":"misc/doc-guide/#code-blocks","text":"Please assign the code language so it can be correctly rendered. For example ``` C for C language. // A function to implement bubble sort void bubbleSort(int arr[], int n) { int i, j; for (i = 0; i < n-1; i++) // Last i elements are already in place for (j = 0; j < n-i-1; j++) if (arr[j] > arr[j+1]) swap(&arr[j], &arr[j+1]); }","title":"Code blocks"},{"location":"misc/doc-guide/#admonitions","text":"You can use !!! Note or !!! Warning or !!! Danger to start a paragraph of admonitions. Then use 4 spaces to start the admonition text. For example !!! Danger \"Error Message\" This is a dangerous error. It will be shown as: Error Message This is a dangerous error.","title":"Admonitions"},{"location":"misc/doc-guide/#deploy-to-github-pages","text":"When most of the edition work is done, and it's time to commit your documents to oc-snap github. First, you should commit and push your changes of source files (in web-doc ) to git repository. Create pull request, ask someone to review the documents, merge them into master branch after getting approvements. Then you can simply publish website with just one step: cd <PATH>/oc-accel/web-doc mkdocs gh-deploy The entire website will be pushed to gh-pages branch of oc-snap repository. The documentation website will be available at https://opencapi.github.io/oc-accel/ !","title":"Deploy to Github Pages"},{"location":"user-guide/0-steps/","text":"Steps in a glance Under \"User-guide\" menu, the pages/steps are organized in this way: If you are new to OC-Accel, please go through (1), (2), and (8). With running helloworld, you will get an idea about the workflow and basic operations quickly. After that, it's time to create your own accelerator (\"action\"). Then go through (1), (3), (4/5), (6), (7), (8). If you already have an action developed in SNAP1/2 and want to move to OC-Accel, please read the notes in (9). Tips and Known issues are being updated in (10).","title":"(0) Steps in a glance"},{"location":"user-guide/0-steps/#steps-in-a-glance","text":"Under \"User-guide\" menu, the pages/steps are organized in this way: If you are new to OC-Accel, please go through (1), (2), and (8). With running helloworld, you will get an idea about the workflow and basic operations quickly. After that, it's time to create your own accelerator (\"action\"). Then go through (1), (3), (4/5), (6), (7), (8). If you already have an action developed in SNAP1/2 and want to move to OC-Accel, please read the notes in (9). Tips and Known issues are being updated in (10).","title":"Steps in a glance"},{"location":"user-guide/1-prepare-env/","text":"This page will introduce the basic environmental requests, tools, and general commands to run OC-Accel flow. Prepare Environment Basic Tools Firstly, you need to have an x86 machine for development with Vivado Tool and the license. export XILINX_VIVADO=<...path...>/Xilinx/Vivado/<VERSION> export XILINXD_LICENSE_FILE=<pointer to Xilinx license> export PATH=$PATH:${XILINX_VIVADO}/bin Note OC-Accel works on Vivado 2018.2, 2018.3 and 2019.1 For AD9H3 and AD9H7 cards with HBM, Vivado version is at least 2018.3 There is a file setup_tools.ksh in the root directory for reference. But for the beginning, only Vivado is required. Make sure you have gcc , make , sed , awk , xterm and python installed. setup_tools.ksh You may install other simulators to accelerate the speed of simulation. For example, Cadence xcelium . See in co-simulation for more information. Clone Github Repositories TODO: Link to update git clone git@github.com:OpenCAPI/oc-accel.git cd oc-accel git submodule init git submodule update cd .. git clone git@github.com:OpenCAPI/ocse.git It's better to have ocse stay in the same directory parallel to oc-accel . That is the default path of $OCSE_ROOT . Or you need to assign $OCSE_ROOT explicitly in snap_env.sh . Basic terms Option1: All-in-one python script OC-Accel developed a \"all-in-one\" Python script to control the workflow. It's convenient to do batch work, or enable your regression verification or continuous integration. cd oc-accel ./ocaccel_workflow.py This script will Check environmental variables make snap_config build model start simulation There are many options provided by ocaccel_workflow.py . Check the help messages by ./ocaccel_workflow.py --help It helps you to do all kinds of operations in one command line. Option2: Traditional \"make\" steps If you have used SNAP for CAPI1.0 and CAPI2.0, you can continue to use these \"traditional\" make steps. Just typing \"make\" doesn't work. An explicit target is needed. You can find them in Makefile file. cd oc-accel make help Main targets for the SNAP Framework make process: ================================================= * snap_config Configure SNAP framework * model Build simulation model for simulator specified via target snap_config * sim Start a simulation * sim_tmux Start a simulation in tmux (no xterm window popped up) * hw_project Create Vivado project with oc-bip * image Build a complete FPGA bitstream after hw_project (takes more than one hour) * hardware One step to build FPGA bitstream (Combines targets 'model' and 'image') * software Build software libraries and tools for SNAP * apps Build the applications for all actions * clean Remove all files generated in make process * clean_config As target 'clean' plus reset of the configuration * help Print this message The hardware related targets 'model', 'image', 'hardware', 'hw_project' and 'sim' do only exist on an x86 platform For simulation make snap_config make model make sim Note After make model , you can continue to run make image to generate bitstreams. In fact, make model also creates a Vivado project framework.xpr in hardware/viv_project . Then it exports the simulation files and compiles them to a simulation model. For Image build make snap_config If it has already been executed, no need to run it again. make hw_project make image Note Use Vivado GUI : After make hw_project , you can open project framework.xpr in hardware/viv_project , and do following \"run Synthesis\" , \"run Implementation\" and \"generate Bitstream\" in Vivado GUI. Output files The log files during these steps are placed in hardware/logs . Simulation output files are placed in hardware/sim/<SIMULATOR>/latest . If you are using make image to generate bitstreams, the outputs are in hardware/build , including Images , Reports and Checkpoints . If you are using Vivado Gui mode to generate bitstream, the outputs are in hardware/viv_project/framework.runs , including synth_1 and impl_1 , etc. Let's go! Now, let's take an example hls_helloworld , and have a look at how it runs step by step. Please continue to read page Run helloworld .","title":"(1) Prepare environment"},{"location":"user-guide/1-prepare-env/#prepare-environment","text":"","title":"Prepare Environment"},{"location":"user-guide/1-prepare-env/#basic-tools","text":"Firstly, you need to have an x86 machine for development with Vivado Tool and the license. export XILINX_VIVADO=<...path...>/Xilinx/Vivado/<VERSION> export XILINXD_LICENSE_FILE=<pointer to Xilinx license> export PATH=$PATH:${XILINX_VIVADO}/bin Note OC-Accel works on Vivado 2018.2, 2018.3 and 2019.1 For AD9H3 and AD9H7 cards with HBM, Vivado version is at least 2018.3 There is a file setup_tools.ksh in the root directory for reference. But for the beginning, only Vivado is required. Make sure you have gcc , make , sed , awk , xterm and python installed. setup_tools.ksh You may install other simulators to accelerate the speed of simulation. For example, Cadence xcelium . See in co-simulation for more information.","title":"Basic Tools"},{"location":"user-guide/1-prepare-env/#clone-github-repositories","text":"TODO: Link to update git clone git@github.com:OpenCAPI/oc-accel.git cd oc-accel git submodule init git submodule update cd .. git clone git@github.com:OpenCAPI/ocse.git It's better to have ocse stay in the same directory parallel to oc-accel . That is the default path of $OCSE_ROOT . Or you need to assign $OCSE_ROOT explicitly in snap_env.sh .","title":"Clone Github Repositories"},{"location":"user-guide/1-prepare-env/#basic-terms","text":"","title":"Basic terms"},{"location":"user-guide/1-prepare-env/#option1-all-in-one-python-script","text":"OC-Accel developed a \"all-in-one\" Python script to control the workflow. It's convenient to do batch work, or enable your regression verification or continuous integration. cd oc-accel ./ocaccel_workflow.py This script will Check environmental variables make snap_config build model start simulation There are many options provided by ocaccel_workflow.py . Check the help messages by ./ocaccel_workflow.py --help It helps you to do all kinds of operations in one command line.","title":"Option1: All-in-one python script"},{"location":"user-guide/1-prepare-env/#option2-traditional-make-steps","text":"If you have used SNAP for CAPI1.0 and CAPI2.0, you can continue to use these \"traditional\" make steps. Just typing \"make\" doesn't work. An explicit target is needed. You can find them in Makefile file. cd oc-accel make help Main targets for the SNAP Framework make process: ================================================= * snap_config Configure SNAP framework * model Build simulation model for simulator specified via target snap_config * sim Start a simulation * sim_tmux Start a simulation in tmux (no xterm window popped up) * hw_project Create Vivado project with oc-bip * image Build a complete FPGA bitstream after hw_project (takes more than one hour) * hardware One step to build FPGA bitstream (Combines targets 'model' and 'image') * software Build software libraries and tools for SNAP * apps Build the applications for all actions * clean Remove all files generated in make process * clean_config As target 'clean' plus reset of the configuration * help Print this message The hardware related targets 'model', 'image', 'hardware', 'hw_project' and 'sim' do only exist on an x86 platform","title":"Option2: Traditional \"make\" steps"},{"location":"user-guide/1-prepare-env/#for-simulation","text":"make snap_config make model make sim Note After make model , you can continue to run make image to generate bitstreams. In fact, make model also creates a Vivado project framework.xpr in hardware/viv_project . Then it exports the simulation files and compiles them to a simulation model.","title":"For simulation"},{"location":"user-guide/1-prepare-env/#for-image-build","text":"make snap_config If it has already been executed, no need to run it again. make hw_project make image Note Use Vivado GUI : After make hw_project , you can open project framework.xpr in hardware/viv_project , and do following \"run Synthesis\" , \"run Implementation\" and \"generate Bitstream\" in Vivado GUI.","title":"For Image build"},{"location":"user-guide/1-prepare-env/#output-files","text":"The log files during these steps are placed in hardware/logs . Simulation output files are placed in hardware/sim/<SIMULATOR>/latest . If you are using make image to generate bitstreams, the outputs are in hardware/build , including Images , Reports and Checkpoints . If you are using Vivado Gui mode to generate bitstream, the outputs are in hardware/viv_project/framework.runs , including synth_1 and impl_1 , etc.","title":"Output files"},{"location":"user-guide/1-prepare-env/#lets-go","text":"Now, let's take an example hls_helloworld , and have a look at how it runs step by step. Please continue to read page Run helloworld .","title":"Let's go!"},{"location":"user-guide/10-tips/","text":"ILA Debug Enable ILA_DEBUG in Kconfig Menu. Add your ILA core. Known issues and TODO Features to be supported: Interrupt Multiple-process supported New boards support AD9H7 AD9H3 N250SoC Cleanup work: Renaming SNAP to OCAC; oc-snap to oc-accel NVME related scripts/files cleanup Enrich example lists: Multiple-process example Ethernet example HBM example","title":"(10) Tips"},{"location":"user-guide/10-tips/#ila-debug","text":"Enable ILA_DEBUG in Kconfig Menu. Add your ILA core.","title":"ILA Debug"},{"location":"user-guide/10-tips/#known-issues-and-todo","text":"Features to be supported: Interrupt Multiple-process supported New boards support AD9H7 AD9H3 N250SoC Cleanup work: Renaming SNAP to OCAC; oc-snap to oc-accel NVME related scripts/files cleanup Enrich example lists: Multiple-process example Ethernet example HBM example","title":"Known issues and TODO"},{"location":"user-guide/2-run-helloworld/","text":"Run hls_helloworld snap_config cd oc-snap ./ocaccel_workflow.py Then a KConfig window will popped up. If it doesn't, check Required tools and search 'kconfig' on the homepage. Select HLS HelloWorld in \"Action Type\". There are some other choices listed in the menu. Please input OCSE_ROOT path. Select xsim (the default simulator). To select a TRUE/FALSE feature, press \"Y\" or \"N\". After everything done, move cursor to \"Exit\". Note This Kconfig menu is editable. If you want to add new features or enrich your own menu, please edit scripts/Kconfig file. Then it starts to execute many steps to build a simulation model. It needs several minutes. While waiting for it, open another terminal tab and try to get familiar with some environmental variables. Open snap_env.sh and check the very basic ones: export ACTION_ROOT=<path_of_oc-accel>/actions/hls_helloworld export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<path_to_ocse>/ocse Simulation ocaccel-workflow.py continues running and prints: SNAP Configured You've got configuration like: ACTION_ROOT /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/oc-snap/actions/hls_helloworld FPGACARD AD9V3 FPGACHIP xcvu3p-ffvc1517-2-e SIMULATOR xsim CAPI_VER opencapi30 OCSE_ROOT ../ocse --------> Environment Check vivado installed as /afs/bb/proj/fpga/xilinx/Vivado/2018.3/bin/vivado gcc installed as /bin/gcc xterm installed as /bin/xterm OCSE path /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/ocse is valid SNAP ROOT /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/oc-snap is valid Environment check PASSED --------> Make the simulation model Runnig ... check ./snap_workflow.make_model.log for details of full progress [CREATE_SNAP_IPs.....] start - [=========== ] 37% Then a Xterm window will popped up. (If it doesn't, check if you have installed it by typing xterm in your terminal.) This Xterm window is where you run your application (software part). You can run anything as many times as you want in the xterm window, just like running in the terminal of a real server with FPGA card plugged. Warning If you want to save the content running in this xterm window, please use script before running any commands. When you exit xterm window, everything is saved to a file -- \"typescript\" is its default name. $ script Script started, file is typescript ...... Run Anything ..... $ exit exit Script done, file is typescript Now let's run application snap_helloworld . It is located in $ACTION_ROOT/sw , where $ACTION_ROOT is <path_of_oc-accel>/actions/hls_helloworld . In the above window, it prints the help messages because it requires two arguments: an input text file and an output text file. We have prepared a script in $ACTION_ROOT/tests/hw_test.sh and you can run it directly. This example is asking FPGA to read the input file from host memory, converting the letters to capital case, and write them back to host memory and save in the output file. Now you have finished the software/hardware co-simulation. Type 'exit' in xterm window. All the output logs, waveforms are in hardware/sim/<simulator>/latest . hdclf154: luyong /afs/bb/u/luyong/p9/nd2/oc_dev/oc-snap/hardware/sim/xsim/latest $ ls debug.log ocse_server.dat snap_helloworld.log tin webtalk.jou xsim.dir xsrun.tcl ocse.log shim_host.dat tCAP top.wdb webtalk.log xsim.jou ocse.parms sim.log terminal.log tout xsaet.tcl xsim.log And you can use following command to open the waveform. xsim top.wdb -gui & On the project scope (hierarchy) panel, the user logic is action_w . Make FPGA bit image In above steps, you actually have finished steps of: make snap_config make model make sim (These targets are introduced in Traditional \"make\" steps ) Now you can generate FPGA image by ./ocaccel_workflow.py --no_configure --no_make_model --no_run_sim --make_image It takes about 2 hours or more. For some big design or bad-timing design, it takes even longer. Check the progress: ./snap_workflow.make_image.log Or hardware/logs/snap_build.log for more detailed logs. For example, ./snap_workflow.make_image.log may tell you: [BUILD IMAGE.........] start 16:58:57 Sat Sep 14 2019 A complete FPGA bitstream build got kicked off. This might take more than an hour depending on the machine used The process may be terminated by pressing <CTRL>-C at any time. After termination it can be restarted later. open framework project 16:59:06 Sat Sep 14 2019 start synthesis with directive: Default 16:59:25 Sat Sep 14 2019 start opt_design with directive: Explore 17:18:58 Sat Sep 14 2019 reload opt_design DCP 17:27:19 Sat Sep 14 2019 start place_design with directive: Explore 17:28:23 Sat Sep 14 2019 start phys_opt_design with directive: Explore 17:42:58 Sat Sep 14 2019 start route_design with directive: Explore 18:04:55 Sat Sep 14 2019 start opt_routed_design with directive: Explore 18:39:01 Sat Sep 14 2019 generating reports 18:57:28 Sat Sep 14 2019 Timing (WNS) -11 ps WARNING: TIMING FAILED, but may be OK for lab use generating bitstreams type: user image 18:58:28 Sat Sep 14 2019 So you can have an estimation of the progress. After it's completed, you can find the FPGA bit image files in hardware/build/Images . The file names have the information of build date/time, action name, card type and timing slack (-11ps here). $ cd hardware/build/Images $ ls oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11.bit oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.prm oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.prm Note A small negative timing slack less than 200ps (set in variable $TIMING_LABLIMIT), is usually acceptable for Lab test, but for product, it's suggested to work out a timing cleaned FPGA image. See Build image for more information. Then go to Deploy on Power Server to see how to download the bitstream and run. Summary Now you understand how to run an existing example. You can use the same method to run other examples in actions directory.","title":"(2) Run helloworld"},{"location":"user-guide/2-run-helloworld/#run-hls_helloworld","text":"","title":"Run hls_helloworld"},{"location":"user-guide/2-run-helloworld/#snap_config","text":"cd oc-snap ./ocaccel_workflow.py Then a KConfig window will popped up. If it doesn't, check Required tools and search 'kconfig' on the homepage. Select HLS HelloWorld in \"Action Type\". There are some other choices listed in the menu. Please input OCSE_ROOT path. Select xsim (the default simulator). To select a TRUE/FALSE feature, press \"Y\" or \"N\". After everything done, move cursor to \"Exit\". Note This Kconfig menu is editable. If you want to add new features or enrich your own menu, please edit scripts/Kconfig file. Then it starts to execute many steps to build a simulation model. It needs several minutes. While waiting for it, open another terminal tab and try to get familiar with some environmental variables. Open snap_env.sh and check the very basic ones: export ACTION_ROOT=<path_of_oc-accel>/actions/hls_helloworld export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<path_to_ocse>/ocse","title":"snap_config"},{"location":"user-guide/2-run-helloworld/#simulation","text":"ocaccel-workflow.py continues running and prints: SNAP Configured You've got configuration like: ACTION_ROOT /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/oc-snap/actions/hls_helloworld FPGACARD AD9V3 FPGACHIP xcvu3p-ffvc1517-2-e SIMULATOR xsim CAPI_VER opencapi30 OCSE_ROOT ../ocse --------> Environment Check vivado installed as /afs/bb/proj/fpga/xilinx/Vivado/2018.3/bin/vivado gcc installed as /bin/gcc xterm installed as /bin/xterm OCSE path /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/ocse is valid SNAP ROOT /afs/vlsilab.boeblingen.ibm.com/data/vlsi/eclipz/c14/usr/luyong/p9nd2/oc_dev/oc-snap is valid Environment check PASSED --------> Make the simulation model Runnig ... check ./snap_workflow.make_model.log for details of full progress [CREATE_SNAP_IPs.....] start - [=========== ] 37% Then a Xterm window will popped up. (If it doesn't, check if you have installed it by typing xterm in your terminal.) This Xterm window is where you run your application (software part). You can run anything as many times as you want in the xterm window, just like running in the terminal of a real server with FPGA card plugged. Warning If you want to save the content running in this xterm window, please use script before running any commands. When you exit xterm window, everything is saved to a file -- \"typescript\" is its default name. $ script Script started, file is typescript ...... Run Anything ..... $ exit exit Script done, file is typescript Now let's run application snap_helloworld . It is located in $ACTION_ROOT/sw , where $ACTION_ROOT is <path_of_oc-accel>/actions/hls_helloworld . In the above window, it prints the help messages because it requires two arguments: an input text file and an output text file. We have prepared a script in $ACTION_ROOT/tests/hw_test.sh and you can run it directly. This example is asking FPGA to read the input file from host memory, converting the letters to capital case, and write them back to host memory and save in the output file. Now you have finished the software/hardware co-simulation. Type 'exit' in xterm window. All the output logs, waveforms are in hardware/sim/<simulator>/latest . hdclf154: luyong /afs/bb/u/luyong/p9/nd2/oc_dev/oc-snap/hardware/sim/xsim/latest $ ls debug.log ocse_server.dat snap_helloworld.log tin webtalk.jou xsim.dir xsrun.tcl ocse.log shim_host.dat tCAP top.wdb webtalk.log xsim.jou ocse.parms sim.log terminal.log tout xsaet.tcl xsim.log And you can use following command to open the waveform. xsim top.wdb -gui & On the project scope (hierarchy) panel, the user logic is action_w .","title":"Simulation"},{"location":"user-guide/2-run-helloworld/#make-fpga-bit-image","text":"In above steps, you actually have finished steps of: make snap_config make model make sim (These targets are introduced in Traditional \"make\" steps ) Now you can generate FPGA image by ./ocaccel_workflow.py --no_configure --no_make_model --no_run_sim --make_image It takes about 2 hours or more. For some big design or bad-timing design, it takes even longer. Check the progress: ./snap_workflow.make_image.log Or hardware/logs/snap_build.log for more detailed logs. For example, ./snap_workflow.make_image.log may tell you: [BUILD IMAGE.........] start 16:58:57 Sat Sep 14 2019 A complete FPGA bitstream build got kicked off. This might take more than an hour depending on the machine used The process may be terminated by pressing <CTRL>-C at any time. After termination it can be restarted later. open framework project 16:59:06 Sat Sep 14 2019 start synthesis with directive: Default 16:59:25 Sat Sep 14 2019 start opt_design with directive: Explore 17:18:58 Sat Sep 14 2019 reload opt_design DCP 17:27:19 Sat Sep 14 2019 start place_design with directive: Explore 17:28:23 Sat Sep 14 2019 start phys_opt_design with directive: Explore 17:42:58 Sat Sep 14 2019 start route_design with directive: Explore 18:04:55 Sat Sep 14 2019 start opt_routed_design with directive: Explore 18:39:01 Sat Sep 14 2019 generating reports 18:57:28 Sat Sep 14 2019 Timing (WNS) -11 ps WARNING: TIMING FAILED, but may be OK for lab use generating bitstreams type: user image 18:58:28 Sat Sep 14 2019 So you can have an estimation of the progress. After it's completed, you can find the FPGA bit image files in hardware/build/Images . The file names have the information of build date/time, action name, card type and timing slack (-11ps here). $ cd hardware/build/Images $ ls oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11.bit oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_primary.prm oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.bin oc_2019_0914_1657_25G_hls_helloworld_noSDRAM_AD9V3_-11_secondary.prm Note A small negative timing slack less than 200ps (set in variable $TIMING_LABLIMIT), is usually acceptable for Lab test, but for product, it's suggested to work out a timing cleaned FPGA image. See Build image for more information. Then go to Deploy on Power Server to see how to download the bitstream and run.","title":"Make FPGA bit image"},{"location":"user-guide/2-run-helloworld/#summary","text":"Now you understand how to run an existing example. You can use the same method to run other examples in actions directory.","title":"Summary"},{"location":"user-guide/3-new-action/","text":"A new git fork The first step before creating a new action is to create a git \"fork\". Now play on the forked git repository: git clone https://github.com/<MY_NAME>/oc-accel git submodule init git submodule update Delete the unnecessary branches, only keep \"master\", and create your own branches. When you want to sync with the original repository, do following steps: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Check status: git remote -v # origin https://github.com/MY_NAME/MY_FORK.git (fetch) # origin https://github.com/MY_NAME/MY_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Do the sync: git fetch upstream git merge upstream/master When you have some fixes and added some new features, create a pull request from your fork to the original repository. It will be reviewed and then the contribution will be merged. Note OC-Accel encourages people to create their own actions and the links will be recommended on README.md . But to keep the repository relatively small and neat, it will not copy every user action design into its original actions folder. Submit an issue or pull request to start the discussion. A new action We have several examples as references. Current action example list is: Name Type Description hdl_example VHDL 512b hdl_example inherited from SNAP1/2. Optional FPGA DDR. hdl_single_engine Verilog 1024b example to send AXI read/write commands. Used to measure bandwidth and latency. No FPGA DDR. hls_helloworld C/C++(HLS) 512b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_memcopy_1024 C/C++(HLS) 1024b example to do memcopy. Enabled FPGA DDR. According to the action category, copy the folder of a proper example from actions and name it. Give it a name and type Step1: make snap_config Select HLS Action - manually set ... or HDL Action - manually set ... in the blue kconfig window. Step2: Edit snap_env.sh , point $ACTION_ROOT to the new action. export ACTION_ROOT=<...path...>/oc-accel/actions/my_new_action export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<...path...>/ocse Step3: Edit software/tools/snap_actions.h , add a row of the new action, with the company/person name, action type ID, and a short description. static const struct actions_tab snap_actions[] = { { \"IBM\", 0x10140000, \"hdl_example in VHDL (512b)\" }, { \"IBM\", 0x10140002, \"hdl_single_engine in Verilog (1024b)\" }, { \"IBM\", 0x10140004, \"UVM test for unit verification (no OCSE and software)\" }, { \"IBM\", 0x10141001, \"HLS Sponge (512b)\" }, { \"IBM\", 0x10141008, \"HLS Hello World (512b)\" }, { \"IBM\", 0x1014100B, \"HLS Memcopy 1024 (1024b)\" }, }; Step4: The Action Type (for example, 0x10140000) should match with following places: actions/<my_new_action>/hw/ actions/<my_new_action>/sw/ Do a grep search and replace them. Understand the workflow Modify the example code (sw, hw and tests) to cook a new action. Understanding the workflow can help quickly identifying what's wrong. These steps are organized in Makefile hardware/Makefile software/Makefile Actions --> $ACTION_ROOT/hw/Makefile $ACTION_ROOT/sw/Makefile When adding a new action, before calling the \"All-in-one\" ocaccel_workflow.py, make sure the make process under $ACTION_ROOT works. cd $ACTION_ROOT/sw make cd $ACTION_ROOT/hw make Above figure shows the steps to make a simulation model. The Action related steps are marked in light orange color. There are also two important tricks: PREPROCESS : it will deal with the files with \"_source\" suffix. That means, \"FILE_source\" will be \"pre-processed\" and converted to \"FILE\". \"FILE\" is a generated one and should not be modified manually, and should not be committed to github either. Build Date/Time, Git Version and Card info will be hardcoded into snap_core logic by \" patch_version.sh \" Start simulation After clean up compiling errors in action sw and action hw, kick off a co-simulation by ./ocaccel_workflow.py","title":"(3) Create a new action"},{"location":"user-guide/3-new-action/#a-new-git-fork","text":"The first step before creating a new action is to create a git \"fork\". Now play on the forked git repository: git clone https://github.com/<MY_NAME>/oc-accel git submodule init git submodule update Delete the unnecessary branches, only keep \"master\", and create your own branches. When you want to sync with the original repository, do following steps: git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git Check status: git remote -v # origin https://github.com/MY_NAME/MY_FORK.git (fetch) # origin https://github.com/MY_NAME/MY_FORK.git (push) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch) # upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push) Do the sync: git fetch upstream git merge upstream/master When you have some fixes and added some new features, create a pull request from your fork to the original repository. It will be reviewed and then the contribution will be merged. Note OC-Accel encourages people to create their own actions and the links will be recommended on README.md . But to keep the repository relatively small and neat, it will not copy every user action design into its original actions folder. Submit an issue or pull request to start the discussion.","title":"A new git fork"},{"location":"user-guide/3-new-action/#a-new-action","text":"We have several examples as references. Current action example list is: Name Type Description hdl_example VHDL 512b hdl_example inherited from SNAP1/2. Optional FPGA DDR. hdl_single_engine Verilog 1024b example to send AXI read/write commands. Used to measure bandwidth and latency. No FPGA DDR. hls_helloworld C/C++(HLS) 512b example to read data from host, convert to CAPITAL case and write back. No FPGA DDR. hls_memcopy_1024 C/C++(HLS) 1024b example to do memcopy. Enabled FPGA DDR. According to the action category, copy the folder of a proper example from actions and name it.","title":"A new action"},{"location":"user-guide/3-new-action/#give-it-a-name-and-type","text":"Step1: make snap_config Select HLS Action - manually set ... or HDL Action - manually set ... in the blue kconfig window. Step2: Edit snap_env.sh , point $ACTION_ROOT to the new action. export ACTION_ROOT=<...path...>/oc-accel/actions/my_new_action export TIMING_LABLIMIT=\"-200\" export OCSE_ROOT=<...path...>/ocse Step3: Edit software/tools/snap_actions.h , add a row of the new action, with the company/person name, action type ID, and a short description. static const struct actions_tab snap_actions[] = { { \"IBM\", 0x10140000, \"hdl_example in VHDL (512b)\" }, { \"IBM\", 0x10140002, \"hdl_single_engine in Verilog (1024b)\" }, { \"IBM\", 0x10140004, \"UVM test for unit verification (no OCSE and software)\" }, { \"IBM\", 0x10141001, \"HLS Sponge (512b)\" }, { \"IBM\", 0x10141008, \"HLS Hello World (512b)\" }, { \"IBM\", 0x1014100B, \"HLS Memcopy 1024 (1024b)\" }, }; Step4: The Action Type (for example, 0x10140000) should match with following places: actions/<my_new_action>/hw/ actions/<my_new_action>/sw/ Do a grep search and replace them.","title":"Give it a name and type"},{"location":"user-guide/3-new-action/#understand-the-workflow","text":"Modify the example code (sw, hw and tests) to cook a new action. Understanding the workflow can help quickly identifying what's wrong. These steps are organized in Makefile hardware/Makefile software/Makefile Actions --> $ACTION_ROOT/hw/Makefile $ACTION_ROOT/sw/Makefile When adding a new action, before calling the \"All-in-one\" ocaccel_workflow.py, make sure the make process under $ACTION_ROOT works. cd $ACTION_ROOT/sw make cd $ACTION_ROOT/hw make Above figure shows the steps to make a simulation model. The Action related steps are marked in light orange color. There are also two important tricks: PREPROCESS : it will deal with the files with \"_source\" suffix. That means, \"FILE_source\" will be \"pre-processed\" and converted to \"FILE\". \"FILE\" is a generated one and should not be modified manually, and should not be committed to github either. Build Date/Time, Git Version and Card info will be hardcoded into snap_core logic by \" patch_version.sh \"","title":"Understand the workflow"},{"location":"user-guide/3-new-action/#start-simulation","text":"After clean up compiling errors in action sw and action hw, kick off a co-simulation by ./ocaccel_workflow.py","title":"Start simulation"},{"location":"user-guide/4-hdl-design/","text":"VHDL/Verilog Action HW Design Take hdl_single_engine as an example, the top design is action_wrapper.v , you need to implement at least axi_lite_slave axi_master axi_lite_slave (Action Registers) Signal Width ADDR 32bits DATA 32bits The Action registers can be defined freely. For a single engine inside an action, the lower 22bits of ADDR are available. That means, you can define one million 4B registers inside 4MB range. The address definition should match with Action SW header file. For example, #define REG_SNAP_CONTROL 0x00 #define REG_SNAP_INT_ENABLE 0x04 #define REG_SNAP_ACTION_TYPE 0x10 #define REG_SNAP_ACTION_VERSION 0x14 // User defined below #define REG_USER_STATUS 0x30 #define REG_USER_CONTROL 0x34 #define REG_USER_MODE 0x38 ..... Note The User defined Action registers are suggested to start from 0x30 . OC-Accel has some pre-defined Action registers in the range of 0x00-0x2F to cooperate with libosnap and software tools. See Deep Dive: Registers for more information. axi_master (Data Path) Here lists the supported AXI feature list from the viewpoint of an Action . When DATA width is chosen to be 512b, a Xilinx IP \"axi_dwidth_converter\" (data width converter) will be inserted automatically ( diagram ), and this converter may not support all of the AXI features. (It also costs FPGA resources!) So we recommend to design a 1024b-wide axi_master and you can use unaligned address and write strobe freely to transfer small sized data. Read more in AXI4 feature list of snap_core. Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. Please drive zero to AXI signals cache , lock , qos , region . Another reason for 1024b-wide axi_master is to make full use of OpenCAPI bandwidth. When an Action runs at 200MHz, 1024b continuous data transferring can get: 1024b * 200MHz = 200Gb/s = 25GB/s And for 512b-wide action with the same clock frequency only half of the possible bandwidth is utilized. And increasing clock frequency may bring difficulties to get timing closure. Action Clock The default clock ap_clk ( clock_act ) frequency is 200MHz, same as the clock feeding snap_core ( clock_afu ). If a different Action Clock is required, please modify hardware/hdl/oc_functions.vhd_source to add a clock generator. The script for this clock generator should be added into hardware/setup/create_snap_ip.tcl . For more information, please read Clock Domain . Action IPs Take hdl_single_engine as an example, actions/hdl_single_engine/hw/Makefile calls a script action_config.sh . Many additional steps can be added in this way to construct the Action hw design, for example, create some IPs: echo \" Call create_action_ip.tcl to generate IPs\" vivado -mode batch -source $ACTION_ROOT/ip/create_action_ip.tcl -notrace -nojournal -tclargs $ACTION_ROOT $FPGACHIP Add action specific IPs into $ACTION_ROOT/ip/create_action_ip.tcl . After they are generated in the Action HW \"make\" process, they will be imported into the project by hardware/setup/create_framework.tcl : # HDL Action IP foreach ip_xci [glob -nocomplain -dir $action_ip_dir */*.xci] { set ip_name [exec basename $ip_xci .xci] puts \" adding HDL Action IP $ip_name\" add_files -norecurse $ip_xci -force >> $log_file export_ip_user_files -of_objects [get_files \"$ip_xci\"] -no_script -sync -force >> $log_file } OC-Accel has enabled one channel of DDR memory controller for AD9V3 card. Take hdl_example (VHDL) and enable \"DDR\" in the KConfig menu. It will ask hardware/setup/create_snap_ip.tcl to generate the required memory controller and simulation model and create_framework.tcl will integrate it. Any other peripheral IPs can be generated similarly. Note Whether to enable a peripheral IP depends on the user case and also depends on the FPGA card. Putting the IP creation script in create_snap_ip.tcl or create_action_ip.tcl are both fine. Action Tcls hardware/setup/create_framework.tcl will also parse $ACTION_ROOT/hw/tcl folder to execute Action tcl scripts. # Action Specific tcl if { [file exists $action_tcl] == 1 } { set tcl_exists [exec find $action_tcl -name *.tcl] if { $tcl_exists != \"\" } { foreach tcl_file [glob -nocomplain -dir $action_tcl *.tcl] { set tcl_file_name [exec basename $tcl_file] puts \" sourcing $tcl_file_name\" source $tcl_file } } } Unit Verification Unit verification is optional and depends on the developers. Co-simulation with OCSE is a good way to verify the correctness, but developers can also build their own unit verification environment to enable more advanced verification tools and methodologies like UVM. Developers and apply more stress random tests and coverage harvest to assure the design quality. VHDL/Verilog Action SW Design The steps include: snap_card_alloc_dev() snap_attach_action() Main body: ACTION registers interaction with hardware snap_detach_action() snap_card_free() About \"main body\" there are a lot of area to explore here. Two APIs are used to read and write Action registers: snap_action_read32() snap_action_write32() Interrupt is supported ( TODO : not fully done yet.) Multiple-process access is also supported. ( TODO : read Examples: hdl_multple_engine)","title":"(4) Verilog/VHDL design"},{"location":"user-guide/4-hdl-design/#vhdlverilog-action-hw-design","text":"Take hdl_single_engine as an example, the top design is action_wrapper.v , you need to implement at least axi_lite_slave axi_master","title":"VHDL/Verilog Action HW Design"},{"location":"user-guide/4-hdl-design/#axi_lite_slave-action-registers","text":"Signal Width ADDR 32bits DATA 32bits The Action registers can be defined freely. For a single engine inside an action, the lower 22bits of ADDR are available. That means, you can define one million 4B registers inside 4MB range. The address definition should match with Action SW header file. For example, #define REG_SNAP_CONTROL 0x00 #define REG_SNAP_INT_ENABLE 0x04 #define REG_SNAP_ACTION_TYPE 0x10 #define REG_SNAP_ACTION_VERSION 0x14 // User defined below #define REG_USER_STATUS 0x30 #define REG_USER_CONTROL 0x34 #define REG_USER_MODE 0x38 ..... Note The User defined Action registers are suggested to start from 0x30 . OC-Accel has some pre-defined Action registers in the range of 0x00-0x2F to cooperate with libosnap and software tools. See Deep Dive: Registers for more information.","title":"axi_lite_slave (Action Registers)"},{"location":"user-guide/4-hdl-design/#axi_master-data-path","text":"Here lists the supported AXI feature list from the viewpoint of an Action . When DATA width is chosen to be 512b, a Xilinx IP \"axi_dwidth_converter\" (data width converter) will be inserted automatically ( diagram ), and this converter may not support all of the AXI features. (It also costs FPGA resources!) So we recommend to design a 1024b-wide axi_master and you can use unaligned address and write strobe freely to transfer small sized data. Read more in AXI4 feature list of snap_core. Note Choose less AXI IDs can save the area of snap_core. The AXI ID ports have at least 1 bit. Drive zero if the Action hardware design doesn't use it. Please drive zero to AXI signals cache , lock , qos , region . Another reason for 1024b-wide axi_master is to make full use of OpenCAPI bandwidth. When an Action runs at 200MHz, 1024b continuous data transferring can get: 1024b * 200MHz = 200Gb/s = 25GB/s And for 512b-wide action with the same clock frequency only half of the possible bandwidth is utilized. And increasing clock frequency may bring difficulties to get timing closure.","title":"axi_master (Data Path)"},{"location":"user-guide/4-hdl-design/#action-clock","text":"The default clock ap_clk ( clock_act ) frequency is 200MHz, same as the clock feeding snap_core ( clock_afu ). If a different Action Clock is required, please modify hardware/hdl/oc_functions.vhd_source to add a clock generator. The script for this clock generator should be added into hardware/setup/create_snap_ip.tcl . For more information, please read Clock Domain .","title":"Action Clock"},{"location":"user-guide/4-hdl-design/#action-ips","text":"Take hdl_single_engine as an example, actions/hdl_single_engine/hw/Makefile calls a script action_config.sh . Many additional steps can be added in this way to construct the Action hw design, for example, create some IPs: echo \" Call create_action_ip.tcl to generate IPs\" vivado -mode batch -source $ACTION_ROOT/ip/create_action_ip.tcl -notrace -nojournal -tclargs $ACTION_ROOT $FPGACHIP Add action specific IPs into $ACTION_ROOT/ip/create_action_ip.tcl . After they are generated in the Action HW \"make\" process, they will be imported into the project by hardware/setup/create_framework.tcl : # HDL Action IP foreach ip_xci [glob -nocomplain -dir $action_ip_dir */*.xci] { set ip_name [exec basename $ip_xci .xci] puts \" adding HDL Action IP $ip_name\" add_files -norecurse $ip_xci -force >> $log_file export_ip_user_files -of_objects [get_files \"$ip_xci\"] -no_script -sync -force >> $log_file } OC-Accel has enabled one channel of DDR memory controller for AD9V3 card. Take hdl_example (VHDL) and enable \"DDR\" in the KConfig menu. It will ask hardware/setup/create_snap_ip.tcl to generate the required memory controller and simulation model and create_framework.tcl will integrate it. Any other peripheral IPs can be generated similarly. Note Whether to enable a peripheral IP depends on the user case and also depends on the FPGA card. Putting the IP creation script in create_snap_ip.tcl or create_action_ip.tcl are both fine.","title":"Action IPs"},{"location":"user-guide/4-hdl-design/#action-tcls","text":"hardware/setup/create_framework.tcl will also parse $ACTION_ROOT/hw/tcl folder to execute Action tcl scripts. # Action Specific tcl if { [file exists $action_tcl] == 1 } { set tcl_exists [exec find $action_tcl -name *.tcl] if { $tcl_exists != \"\" } { foreach tcl_file [glob -nocomplain -dir $action_tcl *.tcl] { set tcl_file_name [exec basename $tcl_file] puts \" sourcing $tcl_file_name\" source $tcl_file } } }","title":"Action Tcls"},{"location":"user-guide/4-hdl-design/#unit-verification","text":"Unit verification is optional and depends on the developers. Co-simulation with OCSE is a good way to verify the correctness, but developers can also build their own unit verification environment to enable more advanced verification tools and methodologies like UVM. Developers and apply more stress random tests and coverage harvest to assure the design quality.","title":"Unit Verification"},{"location":"user-guide/4-hdl-design/#vhdlverilog-action-sw-design","text":"The steps include: snap_card_alloc_dev() snap_attach_action() Main body: ACTION registers interaction with hardware snap_detach_action() snap_card_free() About \"main body\" there are a lot of area to explore here. Two APIs are used to read and write Action registers: snap_action_read32() snap_action_write32() Interrupt is supported ( TODO : not fully done yet.) Multiple-process access is also supported. ( TODO : read Examples: hdl_multple_engine)","title":"VHDL/Verilog Action SW Design"},{"location":"user-guide/5-hls-design/","text":"HLS Action HW Design Take hls_memcopy_1024 as an example, the top design is hardware/hdl/hls/action_wrapper.vhd_source but the HLS developer doesn't need to modify it. That is a common wrapper. hls_action() HLS developer needs to modify $ACTION_ROOT/hw/xxx.CPP , starting from hls_action() : //--- TOP LEVEL MODULE ------------------------------------------------- void hls_action(snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, snap_membus_512_t *d_ddrmem, action_reg *act_reg, action_RO_config_reg *Action_Config) { // Host Memory AXI Interface #pragma HLS INTERFACE m_axi port=din_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=din_gmem bundle=ctrl_reg offset=0x030 #pragma HLS INTERFACE m_axi port=dout_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=dout_gmem bundle=ctrl_reg offset=0x040 // DDR memory Interface #pragma HLS INTERFACE m_axi port=d_ddrmem bundle=card_mem0 offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=d_ddrmem bundle=ctrl_reg offset=0x050 // Host Memory AXI Lite Interface #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 #pragma HLS DATA_PACK variable=act_reg #pragma HLS INTERFACE s_axilite port=act_reg bundle=ctrl_reg offset=0x100 #pragma HLS INTERFACE s_axilite port=return bundle=ctrl_reg ... In Vivado High Level Synthesis, the above C code can be synthesized to have: Host Memory AXI Interface din_gmem dout_gmem DDR memory Interface d_ddrmem AXI Lite Interface Action_Config at offset 0x10 act_reg at offset 0x100 din_gmem and dout_gmem They represent the whole 2^64 Host memory space. But this memory space is shaped as a 128Bytes width , (2^64/128) depth array. With CAPI/OpenCAPI, the action hardware can directly use EA (effective address, the same pointer in software) to access host memory. You will see this shape from the definition of \"snap_membus_1024_t\". snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, Because din_gmem and dout_gmem are shaped as 128B width, the EA address needs to be right-shifted by log2(128) = 7 before being used as din_gmem/dout_gmem's array index. // byte address received need to be aligned with port width InputIndex = (act_reg->Data.in.addr) >> ADDR_RIGHT_SHIFT_1024; OutputIndex = (act_reg->Data.out.addr) >> ADDR_RIGHT_SHIFT_1024; Then you can use memcpy() or data = din_gmem[index] or dout_gmem[index] = data to use them. din_gmem is for reading data from Host. dout_gmem is for writing data to Host. They should be able to be combined just like d_ddrmem but we want to keep the same style as SNAP1/2 for now. d_ddrmem It represents one channel of DDR on FPGA board. In SNAP1/2 and OC-Accel, only one channel (one DDR memory controller) is implemented as an example. However, usually the FPGA board provides more than one DDR channel. There are multiple ways to arrange them so it's user-case dependent. Some FPGA boards have HBM. The developer can enable it similarly like DDR, which is introduced in Deep Dive: New board support . Please be aware of the address range you can use for a single DDR channel. d_ddrmem is shaped as a 64Bytes width , (Capacity/64) depth array. Exceeding the available DDR address range will lead to an unknown error. AXI Lite Registers The first 16bytes (0x00 to 0x0F) are pre-defined by software/include/osnap_hls_if.h , the user is not supposed to change that. It controls the start , stop and interrupt of the HLS Action. The following 8bytes (0x10 to 0x17) is action_RO_config_reg (Action_Config), defined in actions/include/hls_snap*.H , the user should set action_type and release_level information in above two fields. typedef struct { snapu32_t action_type; // 4 bytes snapu32_t release_level; // 4 bytes } action_RO_config_reg; The action_reg has 128bytes (0x100 to 0x180) , composed of two segments Control and job Data , defined in $ACTION_ROOT/hw/*.H typedef struct { CONTROL Control; /* 16 bytes */ memcopy_job_t Data; /* up to 108 bytes */ uint8_t padding[SNAP_HLS_JOBSIZE - sizeof(memcopy_job_t)]; } action_reg; Struct CONTROL is also defined in actions/include/hls_snap*.H , it defines the flags and return code . typedef struct { snapu8_t sat; snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; } CONTROL; At last, what the developer can really freely define is just job Data : memcopy_job_t Data; /* up to 108 bytes */ Note If 108 bytes are not enough, please define a small buffer in Host memory as WED buffer (Work Element Descriptor), store all of the parameters in this WED buffer, and just put the address pointers of this WED buffer into xxxx_job_t . Ask HLS Action to read the content of WED buffer first, then do the following jobs. The above register layout is also drawn in Deep Dive: Registers HLS optimization Xilinx Document UG902: Vivado High-Level Synthesis is an important guide book to understand how to add \"directives\" to your HLS C/C++ code. You can also refer to SNAP1/2 document How to Optimize HLS Action to learn how to run standalone testing before OCSE Co-simulation, and how to fully explore UNROLL and PIPELINE directives in HLS. HLS Action SW Design The steps include: snap_card_alloc_dev() snap_attach_action() Prepare job : snap_set_job() Main body : snap_action_sync_execute_job(), which has three steps: snap_action_sync_execute_job_set_regs (action, cjob); //Set action_reg by MMIO snap_action_start(action); snap_action_sync_execute_job_check_completion (action, cjob, timeout_sec); Check Return code : cjob.retc snap_detach_action() snap_card_free()","title":"(5) HLS C++ design"},{"location":"user-guide/5-hls-design/#hls-action-hw-design","text":"Take hls_memcopy_1024 as an example, the top design is hardware/hdl/hls/action_wrapper.vhd_source but the HLS developer doesn't need to modify it. That is a common wrapper.","title":"HLS Action HW Design"},{"location":"user-guide/5-hls-design/#hls_action","text":"HLS developer needs to modify $ACTION_ROOT/hw/xxx.CPP , starting from hls_action() : //--- TOP LEVEL MODULE ------------------------------------------------- void hls_action(snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, snap_membus_512_t *d_ddrmem, action_reg *act_reg, action_RO_config_reg *Action_Config) { // Host Memory AXI Interface #pragma HLS INTERFACE m_axi port=din_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=din_gmem bundle=ctrl_reg offset=0x030 #pragma HLS INTERFACE m_axi port=dout_gmem bundle=host_mem offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=dout_gmem bundle=ctrl_reg offset=0x040 // DDR memory Interface #pragma HLS INTERFACE m_axi port=d_ddrmem bundle=card_mem0 offset=slave depth=512 \\ max_read_burst_length=64 max_write_burst_length=64 #pragma HLS INTERFACE s_axilite port=d_ddrmem bundle=ctrl_reg offset=0x050 // Host Memory AXI Lite Interface #pragma HLS DATA_PACK variable=Action_Config #pragma HLS INTERFACE s_axilite port=Action_Config bundle=ctrl_reg offset=0x010 #pragma HLS DATA_PACK variable=act_reg #pragma HLS INTERFACE s_axilite port=act_reg bundle=ctrl_reg offset=0x100 #pragma HLS INTERFACE s_axilite port=return bundle=ctrl_reg ... In Vivado High Level Synthesis, the above C code can be synthesized to have: Host Memory AXI Interface din_gmem dout_gmem DDR memory Interface d_ddrmem AXI Lite Interface Action_Config at offset 0x10 act_reg at offset 0x100","title":"hls_action()"},{"location":"user-guide/5-hls-design/#din_gmem-and-dout_gmem","text":"They represent the whole 2^64 Host memory space. But this memory space is shaped as a 128Bytes width , (2^64/128) depth array. With CAPI/OpenCAPI, the action hardware can directly use EA (effective address, the same pointer in software) to access host memory. You will see this shape from the definition of \"snap_membus_1024_t\". snap_membus_1024_t *din_gmem, snap_membus_1024_t *dout_gmem, Because din_gmem and dout_gmem are shaped as 128B width, the EA address needs to be right-shifted by log2(128) = 7 before being used as din_gmem/dout_gmem's array index. // byte address received need to be aligned with port width InputIndex = (act_reg->Data.in.addr) >> ADDR_RIGHT_SHIFT_1024; OutputIndex = (act_reg->Data.out.addr) >> ADDR_RIGHT_SHIFT_1024; Then you can use memcpy() or data = din_gmem[index] or dout_gmem[index] = data to use them. din_gmem is for reading data from Host. dout_gmem is for writing data to Host. They should be able to be combined just like d_ddrmem but we want to keep the same style as SNAP1/2 for now.","title":"din_gmem and dout_gmem"},{"location":"user-guide/5-hls-design/#d_ddrmem","text":"It represents one channel of DDR on FPGA board. In SNAP1/2 and OC-Accel, only one channel (one DDR memory controller) is implemented as an example. However, usually the FPGA board provides more than one DDR channel. There are multiple ways to arrange them so it's user-case dependent. Some FPGA boards have HBM. The developer can enable it similarly like DDR, which is introduced in Deep Dive: New board support . Please be aware of the address range you can use for a single DDR channel. d_ddrmem is shaped as a 64Bytes width , (Capacity/64) depth array. Exceeding the available DDR address range will lead to an unknown error.","title":"d_ddrmem"},{"location":"user-guide/5-hls-design/#axi-lite-registers","text":"The first 16bytes (0x00 to 0x0F) are pre-defined by software/include/osnap_hls_if.h , the user is not supposed to change that. It controls the start , stop and interrupt of the HLS Action. The following 8bytes (0x10 to 0x17) is action_RO_config_reg (Action_Config), defined in actions/include/hls_snap*.H , the user should set action_type and release_level information in above two fields. typedef struct { snapu32_t action_type; // 4 bytes snapu32_t release_level; // 4 bytes } action_RO_config_reg; The action_reg has 128bytes (0x100 to 0x180) , composed of two segments Control and job Data , defined in $ACTION_ROOT/hw/*.H typedef struct { CONTROL Control; /* 16 bytes */ memcopy_job_t Data; /* up to 108 bytes */ uint8_t padding[SNAP_HLS_JOBSIZE - sizeof(memcopy_job_t)]; } action_reg; Struct CONTROL is also defined in actions/include/hls_snap*.H , it defines the flags and return code . typedef struct { snapu8_t sat; snapu8_t flags; snapu16_t seq; snapu32_t Retc; snapu64_t Reserved; } CONTROL; At last, what the developer can really freely define is just job Data : memcopy_job_t Data; /* up to 108 bytes */ Note If 108 bytes are not enough, please define a small buffer in Host memory as WED buffer (Work Element Descriptor), store all of the parameters in this WED buffer, and just put the address pointers of this WED buffer into xxxx_job_t . Ask HLS Action to read the content of WED buffer first, then do the following jobs. The above register layout is also drawn in Deep Dive: Registers","title":"AXI Lite Registers"},{"location":"user-guide/5-hls-design/#hls-optimization","text":"Xilinx Document UG902: Vivado High-Level Synthesis is an important guide book to understand how to add \"directives\" to your HLS C/C++ code. You can also refer to SNAP1/2 document How to Optimize HLS Action to learn how to run standalone testing before OCSE Co-simulation, and how to fully explore UNROLL and PIPELINE directives in HLS.","title":"HLS optimization"},{"location":"user-guide/5-hls-design/#hls-action-sw-design","text":"The steps include: snap_card_alloc_dev() snap_attach_action() Prepare job : snap_set_job() Main body : snap_action_sync_execute_job(), which has three steps: snap_action_sync_execute_job_set_regs (action, cjob); //Set action_reg by MMIO snap_action_start(action); snap_action_sync_execute_job_check_completion (action, cjob, timeout_sec); Check Return code : cjob.retc snap_detach_action() snap_card_free()","title":"HLS Action SW Design"},{"location":"user-guide/6-co-simulation/","text":"Overview of Co-Simulation Co-Simulation lets you run the software (application on the host) together with the hardware (acceleration logic on the FPGA) simulated by functional simulators from either Xilinx Vivado ( xsim ) or other EDA vendors such as Cadence Xcelium ( xrun ), Synopsys VCS ( vcs ), etc. Note All-in-one script ocaccel_workflow.py supports Xilinx Vivado xsim and Cadence Xcelium xrun . It's possible to let make model and make sim flow to support a wider range of simulators. With co-simulation, the functional correctness of your acceleration can be verified with all pieces including software and hardware before you have a real FPGA card to program. Co-simulation in OpenCAPI acceleration framework consists of three processes as shown in the picture below: Process 0 runs RTL simulator for simulating logic behavior of FPGA design Process 1 runs OCSE (OpenCAPI Simulation Engine) Process 2 runs user application. The three processes are communicated via sockets. During the simulation, the user application is exactly the one that will be ran on a real Power9 system, that is to say, for the software designer, co-simulation provides exactly the same behavior as if they are running on a real system with a real FPGA card. In this way, OCSE acts as the proxy to intercept OpenCAPI related requests from user application and re-route them to the RTL simulator, vice versa. The file structure and module level hierarchy is covered in top hierarchy in simulation step . Start Co-Simulation The basic flow of running co-simulation has been covered in simulation section of running helloworld . Please refer to it for a quick getting started. To start the co-simulation, the simplest way is to use ocaccel_workflow.py with its rich command line options. For example, run co-simulation with cadence xcelium where the user application is snap_helloworld : ./ocaccel_workflow.py -o <path to ocse> -s xcelium -t \"snap_helloworld <command line options of snap_helloworld>\" Note In the -t options, ocaccel_workflow.py searches actions/<your action>/sw for the availability of command executable ( snap_helloworld in above example). If the command you are going to run is not in actions/<your action>/sw , please specify the absolute path. or, run co-simulation with vivado xsim without specifying the user application: ./ocaccel_workflow.py -o <path to ocse> -s xsim If the user application is not specified, an xterm will pop up after co-simulation startup. In the xterm , user is expected to run any application they want as if they were running on a real POWER system with an OpenCAPI enabled FPGA card. Note In the popped up xterm , by default the path of the current working directory is where the simulation starts (the simout path). For any command that will trigger access to FPGA card, it should be ran in this simout path, otherwise the attempt of running any of these commands would fail. Check the Simulation Result Running with -t option Warning If there are fatal errors encountered in the RTL simulator or OCSE during simulation, the simulation result will be treated as FAILED regardless of the result of user application, aka, the command specified in -t option. Running without -t option If the test is running without -t option, i.e., running with xterm . Since it is hard to track user behaviors in the xterm (users are expected to run multiple commands with and/or without OpenCAPI traffics), the simulation result is determined by the user themselves. Simulation Output Path Each simulation has a simout path structure as below: <...>/hardware/sim/<simulator>/<timestamp>.<rand_num> For example, if xcelium is used, we have the following directory as the simulation output: <...>/hardware/sim/xcelium/1568471461533.1648652400 All of the simulation related logs, files and waveforms can be found in the simout directory. After each simulation, a soft link named as latest will be created pointing to the simout of the latest simulation. Check the Waveform By default, ocaccel_workflow.py enables waveform dumping. The waveform can be found in the following directory: For Vivado xsim : <...>/hardware/sim/xsim/<timestamp>.<rand_num>/top.wdb For Cadence xcelium : <...>/hardware/sim/xcelium/timestamp>.<rand_num>/capiWave/ To control the simulator specific waveform dumping options (written in tcl), please edit the following files: For Vivado xsim : <...>/hardware/sim/xsaet.tcl For Cadence xcelium : <...>/hardware/sim/ncaet.tcl To disable the waveform dumping, use the following command options in ocaccel_workflow.py : ./ocaccel_workflow.py --no_wave Recompile for small design changes If you have modified the design just a little bit (no file name and hierarchy changed), you can just simply rerun the compile script under: <...>/hardware/sim/xsim/top.sh or <...>/hardware/sim/xcelium/top.sh The simulation will be rebuilt and just run ./run_sim to start the xterm window. It can save time to debug your logic. Warning Be aware of the files having \" _source \" suffixes. If you need to change \"FILE\", please make the modifications on \"FILE_source\", then do following: rm -f FILE cd <...>/hardware make snap_preprocess_execute So \"FILE\" will be regenerated from \"FILE_source\". However, if your design change is big, you should still start from ./ocaccel_workflow.py .","title":"(6) Co-Simulation"},{"location":"user-guide/6-co-simulation/#overview-of-co-simulation","text":"Co-Simulation lets you run the software (application on the host) together with the hardware (acceleration logic on the FPGA) simulated by functional simulators from either Xilinx Vivado ( xsim ) or other EDA vendors such as Cadence Xcelium ( xrun ), Synopsys VCS ( vcs ), etc. Note All-in-one script ocaccel_workflow.py supports Xilinx Vivado xsim and Cadence Xcelium xrun . It's possible to let make model and make sim flow to support a wider range of simulators. With co-simulation, the functional correctness of your acceleration can be verified with all pieces including software and hardware before you have a real FPGA card to program. Co-simulation in OpenCAPI acceleration framework consists of three processes as shown in the picture below: Process 0 runs RTL simulator for simulating logic behavior of FPGA design Process 1 runs OCSE (OpenCAPI Simulation Engine) Process 2 runs user application. The three processes are communicated via sockets. During the simulation, the user application is exactly the one that will be ran on a real Power9 system, that is to say, for the software designer, co-simulation provides exactly the same behavior as if they are running on a real system with a real FPGA card. In this way, OCSE acts as the proxy to intercept OpenCAPI related requests from user application and re-route them to the RTL simulator, vice versa. The file structure and module level hierarchy is covered in top hierarchy in simulation step .","title":"Overview of Co-Simulation"},{"location":"user-guide/6-co-simulation/#start-co-simulation","text":"The basic flow of running co-simulation has been covered in simulation section of running helloworld . Please refer to it for a quick getting started. To start the co-simulation, the simplest way is to use ocaccel_workflow.py with its rich command line options. For example, run co-simulation with cadence xcelium where the user application is snap_helloworld : ./ocaccel_workflow.py -o <path to ocse> -s xcelium -t \"snap_helloworld <command line options of snap_helloworld>\" Note In the -t options, ocaccel_workflow.py searches actions/<your action>/sw for the availability of command executable ( snap_helloworld in above example). If the command you are going to run is not in actions/<your action>/sw , please specify the absolute path. or, run co-simulation with vivado xsim without specifying the user application: ./ocaccel_workflow.py -o <path to ocse> -s xsim If the user application is not specified, an xterm will pop up after co-simulation startup. In the xterm , user is expected to run any application they want as if they were running on a real POWER system with an OpenCAPI enabled FPGA card. Note In the popped up xterm , by default the path of the current working directory is where the simulation starts (the simout path). For any command that will trigger access to FPGA card, it should be ran in this simout path, otherwise the attempt of running any of these commands would fail.","title":"Start Co-Simulation"},{"location":"user-guide/6-co-simulation/#check-the-simulation-result","text":"","title":"Check the Simulation Result"},{"location":"user-guide/6-co-simulation/#running-with-t-option","text":"Warning If there are fatal errors encountered in the RTL simulator or OCSE during simulation, the simulation result will be treated as FAILED regardless of the result of user application, aka, the command specified in -t option.","title":"Running with -t option"},{"location":"user-guide/6-co-simulation/#running-without-t-option","text":"If the test is running without -t option, i.e., running with xterm . Since it is hard to track user behaviors in the xterm (users are expected to run multiple commands with and/or without OpenCAPI traffics), the simulation result is determined by the user themselves.","title":"Running without -t option"},{"location":"user-guide/6-co-simulation/#simulation-output-path","text":"Each simulation has a simout path structure as below: <...>/hardware/sim/<simulator>/<timestamp>.<rand_num> For example, if xcelium is used, we have the following directory as the simulation output: <...>/hardware/sim/xcelium/1568471461533.1648652400 All of the simulation related logs, files and waveforms can be found in the simout directory. After each simulation, a soft link named as latest will be created pointing to the simout of the latest simulation.","title":"Simulation Output Path"},{"location":"user-guide/6-co-simulation/#check-the-waveform","text":"By default, ocaccel_workflow.py enables waveform dumping. The waveform can be found in the following directory: For Vivado xsim : <...>/hardware/sim/xsim/<timestamp>.<rand_num>/top.wdb For Cadence xcelium : <...>/hardware/sim/xcelium/timestamp>.<rand_num>/capiWave/ To control the simulator specific waveform dumping options (written in tcl), please edit the following files: For Vivado xsim : <...>/hardware/sim/xsaet.tcl For Cadence xcelium : <...>/hardware/sim/ncaet.tcl To disable the waveform dumping, use the following command options in ocaccel_workflow.py : ./ocaccel_workflow.py --no_wave","title":"Check the Waveform"},{"location":"user-guide/6-co-simulation/#recompile-for-small-design-changes","text":"If you have modified the design just a little bit (no file name and hierarchy changed), you can just simply rerun the compile script under: <...>/hardware/sim/xsim/top.sh or <...>/hardware/sim/xcelium/top.sh The simulation will be rebuilt and just run ./run_sim to start the xterm window. It can save time to debug your logic. Warning Be aware of the files having \" _source \" suffixes. If you need to change \"FILE\", please make the modifications on \"FILE_source\", then do following: rm -f FILE cd <...>/hardware make snap_preprocess_execute So \"FILE\" will be regenerated from \"FILE_source\". However, if your design change is big, you should still start from ./ocaccel_workflow.py .","title":"Recompile for small design changes"},{"location":"user-guide/7-build-image/","text":"Build the FPGA Bitstream To build FPGA bitstream, you can simply add the following command line option to ocaccel_workflow.py : ./ocaccel_workflow.py --make_image With --make_image , ocaccel_workflow.py will start building image after finishing simulation. If you just want to start building the bitstream without running simulation, please run with the following commands: ./ocaccel_workflow.py --no_make_model --no_run_sim --make_image Check the Result of FPGA Bitstream Basic Result The basic concepts and flows of building FPGA bitstream, including synthesis, place, route and optimization, can be found in Xilinx official documentation ug904 . After finishing the whole bitstream building process, ocaccel_workflow.py will notify the user if vivado is managed to generate bitstream by giving a FAILED or PASSED message. More detailed messaging can be found in ocaccel_workflow.make_image.log in the root directory. If the bitstream is generated, they can be found in the following directory: <ocaccel root>/hardware/build/Images Just take the *.bin or *.bit file in that directory to program the FPGA card according to your own system requirements. Further Debugging Materials if Bitstream Generation Failed If the bitstream is not generated, possibly due to failure of closing timing and/or over-utilization, there are following files for user to further root cause. The reports, including timing report and utilization report: <ocaccel root>/hardware/build/Reports Reports in this directory is generated by Vivado for different stages/phases during the bitstream generation. Please refer to Xilinx Vivado documentation for further explanation. The checkpoints, which allows user to check the status after each stages/phases in Vivado GUI: <ocaccel root>/hardware/build/Checkpoints Specify Bitstream Generation Strategy Vivado has a rich set of strategies/directives for user to choose during bitstream generation (implementation), which has been explained in ug904 . By default, ocaccel_workflow.py uses \" Explore \" as the strategy which means for all the stages/phases, the directive is \" Explore \". You can change this setting by putting a tcl file with strategy settings in the following directory: <oaccel root>/actions/<your action>/hw/tcl/ For example, a file named strategy.tcl can be put in above directory, with the following contents: set_property strategy \"Flow_PerfOptimized_high\" [get_runs synth_1]; set_property strategy \"Performance_NetDelay_high\" [get_runs impl_1]; These settings tell Vivado to use Flow_PerfOptimized_high during synthesis, and Performance_NetDelay_high during implementation. Note The file name doesn't matter in this case. ocaccel_workflow.py is going to source any tcl file in <ocaccel root>actions/<your action>/hw/tcl/ directory.","title":"(7) Build image"},{"location":"user-guide/7-build-image/#build-the-fpga-bitstream","text":"To build FPGA bitstream, you can simply add the following command line option to ocaccel_workflow.py : ./ocaccel_workflow.py --make_image With --make_image , ocaccel_workflow.py will start building image after finishing simulation. If you just want to start building the bitstream without running simulation, please run with the following commands: ./ocaccel_workflow.py --no_make_model --no_run_sim --make_image","title":"Build the FPGA Bitstream"},{"location":"user-guide/7-build-image/#check-the-result-of-fpga-bitstream","text":"","title":"Check the Result of FPGA Bitstream"},{"location":"user-guide/7-build-image/#basic-result","text":"The basic concepts and flows of building FPGA bitstream, including synthesis, place, route and optimization, can be found in Xilinx official documentation ug904 . After finishing the whole bitstream building process, ocaccel_workflow.py will notify the user if vivado is managed to generate bitstream by giving a FAILED or PASSED message. More detailed messaging can be found in ocaccel_workflow.make_image.log in the root directory. If the bitstream is generated, they can be found in the following directory: <ocaccel root>/hardware/build/Images Just take the *.bin or *.bit file in that directory to program the FPGA card according to your own system requirements.","title":"Basic Result"},{"location":"user-guide/7-build-image/#further-debugging-materials-if-bitstream-generation-failed","text":"If the bitstream is not generated, possibly due to failure of closing timing and/or over-utilization, there are following files for user to further root cause. The reports, including timing report and utilization report: <ocaccel root>/hardware/build/Reports Reports in this directory is generated by Vivado for different stages/phases during the bitstream generation. Please refer to Xilinx Vivado documentation for further explanation. The checkpoints, which allows user to check the status after each stages/phases in Vivado GUI: <ocaccel root>/hardware/build/Checkpoints","title":"Further Debugging Materials if Bitstream Generation Failed"},{"location":"user-guide/7-build-image/#specify-bitstream-generation-strategy","text":"Vivado has a rich set of strategies/directives for user to choose during bitstream generation (implementation), which has been explained in ug904 . By default, ocaccel_workflow.py uses \" Explore \" as the strategy which means for all the stages/phases, the directive is \" Explore \". You can change this setting by putting a tcl file with strategy settings in the following directory: <oaccel root>/actions/<your action>/hw/tcl/ For example, a file named strategy.tcl can be put in above directory, with the following contents: set_property strategy \"Flow_PerfOptimized_high\" [get_runs synth_1]; set_property strategy \"Performance_NetDelay_high\" [get_runs impl_1]; These settings tell Vivado to use Flow_PerfOptimized_high during synthesis, and Performance_NetDelay_high during implementation. Note The file name doesn't matter in this case. ocaccel_workflow.py is going to source any tcl file in <ocaccel root>actions/<your action>/hw/tcl/ directory.","title":"Specify Bitstream Generation Strategy"},{"location":"user-guide/8-deploy/","text":"Deploy to Power Server Program FPGA There are two ways to program FPGA: Program Flash Program FPGA chip Program Flash This is the default way to program FPGA. Log on to Power9 server, TODO : link to update $ git clone https://github.ibm.com/OC-Enablement/oc-utils/ $ make $ sudo make install Copy the generated hardware/build/Images/*.bin from the development machine to Power9 server, and execute: sudo oc-flash-script <file.bin> or for SPIx8 flash interface: $ sudo oc-flash-script <file_primary.bin> <file_secondary.bin> If the previous flashing is not done correctly, you may need to delete the lock file manually. $ rm -rf \"/var/cxl/capi-flash-script.lock\" Reboot system. Check if the device is valid by $ ls /dev/ocxl IBM,oc-snap.0007:00:00.1.0 Warning oc-reset hasn't been verified. Program FPGA chip Not like \"programming flash\" which permanently stores FPGA image into the flash on the FPGA board, programming FPGA chip is a temporal method and mainly used for debugging purpose. It uses *.bit file and the programmed image will be lost if the server is powered off. Prepare a laptop/desktop machine and install Vivado Lab . Use USB cable to connect it to the FPGA board's USB-JTAG debugging port. Then in Vivado Lab, right click the FPGA device name and select \"program device...\" For ZZ systems: $ sudo su $ echo 1 > /sys/class/ocxl/IBM\\,oc-snap.0007\\:00\\:00.1.0/reset_adapter For Mihawk/AC922 systems: Warning TODO Install libocxl See https://github.com/OpenCAPI/libocxl/ Compile OC-Accel software and actions $ git clone https://github.com/<MY_NAME>/oc-accel $ make apps You can check the FPGA image version, name and build date/time by $ cd software/tools $ sudo ./snap_maint -vvv [main] Enter [snap_open] Enter: IBM,oc-snap [snap_open] Exit 0x141730670 [snap_version] Enter SNAP Card Id: 0x31 Name: AD9V3. NVME disabled, 0 MB DRAM available. (Align: 1 Min_DMA: 1) SNAP FPGA Release: v0.2.0 Distance: 255 GIT: 0x12fb0b24 SNAP FPGA Build (Y/M/D): 2019/09/13 Time (H:M): 11:24 [snap_version] Exit [snap_action_info] Enter Short | Action Type | Level | Action Name ------+--------------+-----------+------------ 0 0x10140002 0x00000002 IBM hdl_single_engine in Verilog (1024b) [snap_action_info] Exit rc: 0 [main] Exit rc: 0 [snap_close] Enter [snap_close] Exit 0 Run Application $ cd actions/<my_new_action>/sw $ sudo ./<app_name> Note Whenever calling the FPGA card, sudo is needed.","title":"(8) Deploy on Power Server"},{"location":"user-guide/8-deploy/#deploy-to-power-server","text":"","title":"Deploy to Power Server"},{"location":"user-guide/8-deploy/#program-fpga","text":"There are two ways to program FPGA: Program Flash Program FPGA chip","title":"Program FPGA"},{"location":"user-guide/8-deploy/#program-flash","text":"This is the default way to program FPGA. Log on to Power9 server, TODO : link to update $ git clone https://github.ibm.com/OC-Enablement/oc-utils/ $ make $ sudo make install Copy the generated hardware/build/Images/*.bin from the development machine to Power9 server, and execute: sudo oc-flash-script <file.bin> or for SPIx8 flash interface: $ sudo oc-flash-script <file_primary.bin> <file_secondary.bin> If the previous flashing is not done correctly, you may need to delete the lock file manually. $ rm -rf \"/var/cxl/capi-flash-script.lock\" Reboot system. Check if the device is valid by $ ls /dev/ocxl IBM,oc-snap.0007:00:00.1.0 Warning oc-reset hasn't been verified.","title":"Program Flash"},{"location":"user-guide/8-deploy/#program-fpga-chip","text":"Not like \"programming flash\" which permanently stores FPGA image into the flash on the FPGA board, programming FPGA chip is a temporal method and mainly used for debugging purpose. It uses *.bit file and the programmed image will be lost if the server is powered off. Prepare a laptop/desktop machine and install Vivado Lab . Use USB cable to connect it to the FPGA board's USB-JTAG debugging port. Then in Vivado Lab, right click the FPGA device name and select \"program device...\" For ZZ systems: $ sudo su $ echo 1 > /sys/class/ocxl/IBM\\,oc-snap.0007\\:00\\:00.1.0/reset_adapter For Mihawk/AC922 systems: Warning TODO","title":"Program FPGA chip"},{"location":"user-guide/8-deploy/#install-libocxl","text":"See https://github.com/OpenCAPI/libocxl/","title":"Install libocxl"},{"location":"user-guide/8-deploy/#compile-oc-accel-software-and-actions","text":"$ git clone https://github.com/<MY_NAME>/oc-accel $ make apps You can check the FPGA image version, name and build date/time by $ cd software/tools $ sudo ./snap_maint -vvv [main] Enter [snap_open] Enter: IBM,oc-snap [snap_open] Exit 0x141730670 [snap_version] Enter SNAP Card Id: 0x31 Name: AD9V3. NVME disabled, 0 MB DRAM available. (Align: 1 Min_DMA: 1) SNAP FPGA Release: v0.2.0 Distance: 255 GIT: 0x12fb0b24 SNAP FPGA Build (Y/M/D): 2019/09/13 Time (H:M): 11:24 [snap_version] Exit [snap_action_info] Enter Short | Action Type | Level | Action Name ------+--------------+-----------+------------ 0 0x10140002 0x00000002 IBM hdl_single_engine in Verilog (1024b) [snap_action_info] Exit rc: 0 [main] Exit rc: 0 [snap_close] Enter [snap_close] Exit 0","title":"Compile OC-Accel software and actions"},{"location":"user-guide/8-deploy/#run-application","text":"$ cd actions/<my_new_action>/sw $ sudo ./<app_name> Note Whenever calling the FPGA card, sudo is needed.","title":"Run Application"},{"location":"user-guide/9-migrate/","text":"Migrate from SNAP1.0/2.0 Data width change The AXI data port width of OC-Accel is 1024bit. But the actions developed in SNAP (CAPI1.0/2.0) use 512b. Please select 512b in Kconfig Menu. Then a dwidth_converter will be inserted automatically. All of the AXI4 features in SNAP are supported by OC-Accel, and it has added more. Read OC-Accel AXI4 feature list for more details. Clock frequency The default clock frequency in SNAP (CAPI1.0/2.0) was 250MHz . There was no asynchronous logic between capi2-bsp, snap_core and action_wrapper so the frequency had to be adjusted together. The default clock frequency for action_wrapper in OC-Accel is 200MHz . Asynchronous clocks have been designed for oc-bip, snap_core and action_wrapper so the clock frequency can be adjusted more flexibly for each part. See Clock domains . Library name SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) libcxl libocxl libsnap libosnap Included headers SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_types.h osnap_types.h snap_tools.h osnap_tools.h snap_queue.h osnap_queue.h snap_internal.h osnap_internal.h snap_hls_if.h osnap_hls_if.h snap_m_regs.h osnap_global_regs.h snap_s_regs.h N/A snap_regs.h N/A libsnap.h libosnap.h There is a big change of the Register Map. OC-Accel has simplified and enlarged the Register Layout. SNAP1/2 Register map OC-Accel Register map API changes SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_mmio_read32() snap_action_read32() snap_mmio_write32() snap_action_write32() snap_mmio_read64() snap_global_read64() snap_mmio_write64() snap_global_write64() The API changes also reflect the Register map changes. SNAP_CONFIG=CPU discarded In SNAP (CAPI1.0/2.0), it has implemented a group of function pointers for CPU to emulate the FPGA action, aka \"software action\". It is enabled when setting SNAP_CONFIG=CPU : /* Software version of the lowlevel functions */ static struct snap_funcs software_funcs = { .card_alloc_dev = sw_card_alloc_dev, .attach_action = sw_attach_action, /* attach Action */ .detach_action = sw_detach_action, /* detach Action */ .mmio_write32 = sw_mmio_write32, .mmio_read32 = sw_mmio_read32, .mmio_write64 = sw_mmio_write64, .mmio_read64 = sw_mmio_read64, .card_free = sw_card_free, .card_ioctl = sw_card_ioctl, }; These functions have been deleted. The original purpose of SNAP_CONFIG=CPU is to provide a way to fall back to software execution when FPGA is not available. However, this actually can be easily done by higher level of application control, for example: if (!snap_card_alloc_dev()) //Failed to open FPGA card call_original_software_function So there is no need to rewrite the original software function at all. The corresponding concept is SNAP_CONFIG=FPGA and it becomes the ONLY working mode in OC-Accel. So the variable SNAP_CONFIG has been deleted.","title":"(9) Migrate from SNAP1/2"},{"location":"user-guide/9-migrate/#migrate-from-snap1020","text":"","title":"Migrate from SNAP1.0/2.0"},{"location":"user-guide/9-migrate/#data-width-change","text":"The AXI data port width of OC-Accel is 1024bit. But the actions developed in SNAP (CAPI1.0/2.0) use 512b. Please select 512b in Kconfig Menu. Then a dwidth_converter will be inserted automatically. All of the AXI4 features in SNAP are supported by OC-Accel, and it has added more. Read OC-Accel AXI4 feature list for more details.","title":"Data width change"},{"location":"user-guide/9-migrate/#clock-frequency","text":"The default clock frequency in SNAP (CAPI1.0/2.0) was 250MHz . There was no asynchronous logic between capi2-bsp, snap_core and action_wrapper so the frequency had to be adjusted together. The default clock frequency for action_wrapper in OC-Accel is 200MHz . Asynchronous clocks have been designed for oc-bip, snap_core and action_wrapper so the clock frequency can be adjusted more flexibly for each part. See Clock domains .","title":"Clock frequency"},{"location":"user-guide/9-migrate/#library-name","text":"SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) libcxl libocxl libsnap libosnap","title":"Library name"},{"location":"user-guide/9-migrate/#included-headers","text":"SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_types.h osnap_types.h snap_tools.h osnap_tools.h snap_queue.h osnap_queue.h snap_internal.h osnap_internal.h snap_hls_if.h osnap_hls_if.h snap_m_regs.h osnap_global_regs.h snap_s_regs.h N/A snap_regs.h N/A libsnap.h libosnap.h There is a big change of the Register Map. OC-Accel has simplified and enlarged the Register Layout. SNAP1/2 Register map OC-Accel Register map","title":"Included headers"},{"location":"user-guide/9-migrate/#api-changes","text":"SNAP (CAPI1.0/2.0) OC-Accel (OpenCAPI3.0) snap_mmio_read32() snap_action_read32() snap_mmio_write32() snap_action_write32() snap_mmio_read64() snap_global_read64() snap_mmio_write64() snap_global_write64() The API changes also reflect the Register map changes.","title":"API changes"},{"location":"user-guide/9-migrate/#snap_configcpu-discarded","text":"In SNAP (CAPI1.0/2.0), it has implemented a group of function pointers for CPU to emulate the FPGA action, aka \"software action\". It is enabled when setting SNAP_CONFIG=CPU : /* Software version of the lowlevel functions */ static struct snap_funcs software_funcs = { .card_alloc_dev = sw_card_alloc_dev, .attach_action = sw_attach_action, /* attach Action */ .detach_action = sw_detach_action, /* detach Action */ .mmio_write32 = sw_mmio_write32, .mmio_read32 = sw_mmio_read32, .mmio_write64 = sw_mmio_write64, .mmio_read64 = sw_mmio_read64, .card_free = sw_card_free, .card_ioctl = sw_card_ioctl, }; These functions have been deleted. The original purpose of SNAP_CONFIG=CPU is to provide a way to fall back to software execution when FPGA is not available. However, this actually can be easily done by higher level of application control, for example: if (!snap_card_alloc_dev()) //Failed to open FPGA card call_original_software_function So there is no need to rewrite the original software function at all. The corresponding concept is SNAP_CONFIG=FPGA and it becomes the ONLY working mode in OC-Accel. So the variable SNAP_CONFIG has been deleted.","title":"SNAP_CONFIG=CPU discarded"}]}